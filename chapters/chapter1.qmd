# Replication crisis in psychology

In this chapter we outline key methodological and conceptual issues underlying the replication crisis, introducing the concept of replication and its role in science, then addressing major problems such as low replication rates, weak theories, poor measurement, and misleading statistical practices. After delimiting the inferential paradigm, we will then focus on the misuse of the inferential paradigm, especially the overreliance on Null Hypothesis Significance Testing (NHST).

## Introducing the replication crisis

In recent years, psychology has faced a credibility crisis, raising doubts about the credibility of many published findings [@ioannidis2005most]. Since the beginning of such crisis, significant efforts have been devoted to improving data analysis, theory testing, and Open Practices [@lakens2019value; @nosek2015promoting; @oberauer2019addressing; @nosek2014registered]. 

We align with the interventionist perspective that identifies superficial methodological choices (specifically, the neglect of rigorous sample size planning and clinically meaningful effect size interpretation) as one of the main causes of the crisis. In this thesis, we will develop and substantiate this explanation, arguing that a principled application of the Neyman-Pearson framework provides the necessary corrective to these methodological shortcomings.

## What is a replication

"Scientific progress is a cumulative process of uncertainty reduction that can only succeed if science itself remains the greatest skeptic of its explanatory claims" [@abraham52aarts, p.7].

Arguably, the very foundation of science depends on the replication of studies [@murphy2025]. The greater our ability to replicate findings over time, the more confidence we gain in our beliefs [@schmidt2009; @open2015]. Therefore, replication allows us to be more skeptical about certain theories [@eronen2021], while also more confident in confirming and generalizing others [@cohen1994; @schmidt2009].

It is important to define terms that are often used interchangeably in the literature, despite referring to distinct concepts. A *reproduction* study consists in observing the same results, performing the same statistical analysis with the same data (indeed, it's also often referred to as "computational reproducibility"). In contrast, *replication* study consists in conducting the same study with different data and obtaining similar results [@nosek2022replicability]. The main challenge in replication studies is to determine whether the results we obtain are sufficiently close to the original ones, to consider the replication successfull.

Replication is often divided in two categories. Replication that aims to closely match the original study is called *direct replication.* Direct replication are very useful to confidently identify false positive [@nosek2016registered]. On the contrary, when variations are introduced in the study (e.g., different population), this is reffered to as *conceptual replication* [@derksen2022kinds]. Conceptual replication may be useful to explore if the phenomenon can occur in different scenarios, thereby enhacing our comprehension of the effect, and offering more support to the theory [@nosek2016registered]. However, some argue that conceptual replications often attribute the effect to changes in study design rather than challenging the phenomenon itself [@nosek2016registered]. Offering a different perspective on the matter, Machery (2020) proposes abandoning the direct/conceptual distinction, and proposes instead to classify replications on how many components are resampled (e.g., units, treatments, measurements, settings): each combination of resampled components examines a different aspect of the original experiment's reliability.

## Replication crisis

In recent years, many studies have shown low replication rates in psychological research. This has led to the conclusion that many psychological findings may actually be false [@ioannidis2005most; @malich2022 ; @machery2020 ; @oberauer2019addressing ; @open2015 ; @vazire2022credibility].

The main reason why positive results can be easily produced, and therefore published, is due to some inappropriate methodological practices: the most relevant among these are the misuse of an inferential procedure known as the Null Hypothesis Significance Test (NHST), and a range of questionable research practices (QRPs). A larger overview regarding of these two phenomena will be proveided in the dedicated section [@head2015extent ; @scheel2022most; @ioannidis2005most].

The spread of false positives is worsened by a publication bias. Publications offering significant results tend to be published more frequently [@scheel2021]. There's a substantial pressure to publish, and studies presenting significant effects are often published at higher rate [@primbs2023; @banks2016questions]. At the same time, likely due to the inherent difficulty in replicating studies in psychology [@hall2023replication], scientific publishing world prioritizes novelty over replication [@open2015]. Which contributes to the dissemination of low-quality or non-replicable findings.

This controversial finding undermines the very foundation of science, as replication is typically the primary mean by which we identify theories that are likely true. However, since replication is rarely done in psychology, overlapping theories that have not been decisively falsified yet remain common in the field [@eronen2021].

## A broad conceptual crisis

The replication crisis has drawn attention to additional issues that characterize the field of psychology, many of which originate from the discipline's conceptual foundations and are worth noting: the validity crisis and the theory crisis. These aspects are broad and have been extensively discussed elsewhere. Here, a brief explanation will be provided, for the sake of completeness, before proceeding with the discussion of the replication crisis.

### Validity Crisis

Validity crisis concerns the precise definition of the object of study within a given field [@eronen2021]. If researchers are unable to define what they aim to measure, it will be unclear whether they will have been measuring the intended construct in the first place [@flake2020measurement]. In recent years, in response to the replication crisis, increasing attention has been given to the foundations of psychological measurement, as many studies fail to provide clear definitions of the constructs of interest [@vazire2022credibility]. Earlier, Flake and Fried [@flake2020measurement] introduced the concept of *questionable measurement practices* (QMP), addressing all those practices that cast doubt on the validity of measurement, such as lack of transparency in how measures were used and superficial or inappropriate application of measurement tools.

Following the recognition of this crisis, several recommendations have been proposed to improve measurement quality. For instance, Flake [@flake2020measurement] emphasizes the need for greater transparency, and provides a structured series of questions to help researchers define construct and avoid QMP.

The crisis of measurement validity is also reflected in the weakness of psychological theories, which are often based on poorly defined constructs.

### Theory crisis

Theory crisis refers to "a weak logical link between theories and their empirical test" [@oberauer2019addressing, p.2]. This is arguably a more fundamental problem: theories are not strong enough to derive strong hypotheses or to predict the dimension of any effect [@eronen2021; @fried2020lack]. Without strong hypotheses, theories cannot be falsified. Moreover weak theories can easily be defended after the results are known by adding auxiliary hypotheses. A common line of defense is the appeal to "hidden moderators", which are often invoked to explain failed replication [@fried2020lack]. If that is the case though, we are not able to tell whether data corroborates our theory or not.

As Eronen and Bringmann highlighted we should invest more energy into building stronger and more precise theories [@eronen2021]. However psychology is a challenging field, and to build strong theories we first need to identify clear phenomena. Therefore, some argue they should focus more on exploratory research [@fife2022understanding; @oberauer2019addressing], and in phenomenon-driven research, as it helps constrain the space of plausible theories [@eronen2021].

While the validity crisis and the theory crisis certainly warrant further investigation to strengthen psychology's conceptual foundations, the replication crisis is more commonly attributed to other factors. The two main areas of concern are questionable research practices and misunderstandings related to inferential methodology.

## Questionable Research Practices

Questionable Research Practices (QRPs). QRPs are all the methods implemented in the field that aim to obtain a significant value, such as changing the hypothesis after the results are known (often referred to as "HARKing"), stopping data collection after achieving the desired result (often refferred to as "optional stopping"), collecting more data after seeing whether results were significant, manipulating statistical analysis in order to obtain significance (often referred to as "*p*-hacking"), presenting the results of a study that best support the hypothesis instead of reporting all the findings (oftentimes also called "cherry picking") and many others [@scheel2021; @john2012measuring]. Therefore, QRPs include unethical and ambiguous practices, and have been shown to increase the likelihood of obtaining significant results [@john2012measuring], and increase the spread of false positive [@flake2020measurement]. Although often not driven by deliberate intent, QRPs are typically fueled by self-serving biases and the strong pressure to produce significant findings [@john2012measuring; @simmons2011false].

To improve the situation, several solutions have been recently proposed, including strengthening statistical standards, conducting high-powered replications, providing open data, materials and algorithms to enhace reproducibility, and clearly distinguishing a priori hypothesis through preregistration (a practice aimed at preventing HARKing and clarifying the distinction between confirmatory and exploratory research) [@oberauer2019addressing ; @nosek2015promoting]

Significant results, however, refer primarily to outcomes derived from the most widely used statistical framework in psychology: null hypothesis significance testing (NHST) [@head2015extent]. A brief clarification is useful here to understand its role in the replication crisis.

## Inferential framework and its misunderstanding

To address the methodological issues at the core of the replication crisis, we will first specify the inferential framework we are adopting. Then, we will outline the Neyman-Pearson approach, as it is useful for understanding the weaknesses of the aforementioned NHST.

### Frequentist paradigm

As previously noted, we must first clarify the logic used to interpret empirical evidence. Among all the approaches used in inferential sciences, two in particular are mostly used, namely frequentist approach and bayesian approach. Although this thesis works within a frequentist framework, both approaches are sound; see Van de Schoot et al. (2014) for an for an overview of the Bayesian perspective.

In a frequentist perspective, probability is defined as the frequency (i.e., number of occurences) of an event in a set period of time [@vandenbos2007apa]. Having set the inferential paradigm of reference, we will now outline the problems that have emerged in the context of hypothesis testing, and which solutions have been proposed.

### Neyman and Pearson approach

Within this inferential framework, several methodologies exist for hypothesis testing. One of the most studied is the Neyman-Pearson approach.

In the Neyman-Pearson approach, before collecting data, a predetermined level of inferential risk is established in order to make a decision between different hypotheses defined a priori. The test result will allow one to act as if one of the two hypotheses were true and the other false, given a certain level of risk. To do this, one must decide in advance on the following elements [@gigerenzer2004mindless]:

1) Defining two opposing hypothesis. The null hypothesis (H~0~), which usually assumes no effect or difference, and the alternative hypothesis (H~1~), which posits the presence of an effect and is the focus of testing. The magnitude of the effect has to be decided basing on theoretical or practical criteria. By defining H~0~ and H~1~, the associated sampling distributions are also specified. This allows for the division of the sample space into acceptance and rejection regions for each hypothesis. 
2) To define the risks associated with the test. To do so, we specify in advance values for $\alpha$ (the probability of incorrectly rejecting the null hypothesis when it is true, namely Type I error) and $\beta$ (the probability of failing to reject the null hypothesis when it's false). Power can be than calculated as the probability that the test has to reject the null hypothesis (H~0~) when the alternative hypothesis (H~1)~ is true [@altoe2020enhancing]. The level of risk one intends to accept depends on the context and the phenomenon under investigation [@maier2022justify]. 
3) Define the sample size.  Once we know the a priori effect size, and we determine the values for $\alpha$ and $\beta$, the sample size is consequently determined in a design analysis.

The test then yields a *p*-value, which is the probability of observing the same or higher data than observed ones, if H0 is true. The *p*-value is then compared against the pre-specified $\alpha$ (therefore also called "significance level"), and we either accept H~1~ and reject H~0~, or reject H~1~ and accept H~0~ [@neyman1957inductive].

The Neyman and Pearson actually originates from an other approach, the Fisher approach [@fisher1955statistical]. This approach only yields the *p*-value under  H~0~, so the smaller the *p*-value, the more the data are unlikely under the Null Hypohtesis. This approach though is intended for very preliminary analyses, and results must be interpreted basing on the context at hand [@gigerenzer2004mindless]. 
Specifying the usage and limitation of these procedures should make it easy to clarify the issues related to the previously noted NHST, which we will outline in the following section.

### NHST approach

During its history, the development of psychology as a field has led to the widespread use of a simplified and less rigorous variation of this method, the aforementioned Null Hypothesis Significance Testing (NHST) [@gigerenzer2004mindless]. This test represents a hybrid form of the Neyman-Pearson approach and the earlier approach developed by Fisher [@fisher1955statistical]. The NHST paradigm works as follows [@gigerenzer2004mindless]: 

- H~0~ is defined as a null hypothesis (e.g., we test against the complete absence of an effect).
- No alternative hypothesis is specified.
- $\alpha$ is conventionally and uncritically set at 0.05. 
- Finally, if *p*-value < 0.05 (statistically significant), the null hypothesis is rejected. 

As previously noted, this methodology constitutes a simplified and weaker version of the Neyman-Pearson framework, as it omits two of its essential components. First, the significance level ($\alpha$) is commonly fixed at 5% without theoretical justification, making it an uncritical convention rather than a reasoned decision. In fact, it may be more important to identify false negatives more precisely in different context, e.g. a false positive that leads to a useless surgery may arguably be worse than uselessly having some therapy sessions, where we may accept higher risk for false positive. Second, since no alternative hypothesis is specified, no hypothesis can ultimately be accepted. This absence also prevents the definition of an ideal sample size in advance, often leading to under-powered studies [@gigerenzer2004mindless]. Moreover, without an alternative hypothesis, the a priori dimension of the effect cannot be calculated, making the interpretation more misleading.

To these characteristics, one must add a series of further elements of concern, as the NHST approach is characterized by a widespread misconception about logic of hypothesis testing and interpretation of *p*-values [@lakens2021practical ; @fife2022understanding]. See Lakens (2016) for a more detailed discussion. 

A vast majority of misconceptions consists in attributing to *p*-values informative strength over the theory to be tested [@gigerenzer2004mindless]. In fact, once the test yields a significant result, the research hypothesis, which had not been previously specified, is usually considered confirmed. This is incorrect, as no alternative theory had been previously specified. On the other hand, if a non-significant result is obtained, most psychologists often conclude that there is no effect, which is equally false [@lakens2017equivalence]. Truly, a significant result does not indicate that a theory is likely true; it only tells us how likely the observed data are, assuming the null hypothesis is true [@cohen1994], since, as Gingerenzer states, "The probability *P*(*D*|H~0~) is not the same as *P*(H~0~|*D*), and more generally, a significance test does not provide a probability for a hypothesis" (2004, p.95). Ultimately, *p*-value is often erroneously considered as the strength of an effect or relationship [@head2015extent].

In synthesis, even when properly understood, NHST presents some unavoidable weaknesses. It only allows for the rejection of the null hypothesis, offering little to no meaningful information about the interpretation of the effect [@cohen1994]. Moreover, since population parameters in the real world are virtually never exactly zero, even negligible differences can become statistically significant with sufficiently large sample sizes [@lin2013research]. This can lead to a misleading sense of confirmation for effects that are practically irrelevant [@malich2022; @lin2013research]. Additionally, by omitting the alternative hypothesis and the corresponding effect size, NHST prevents the calculation of an appropriate sample size to achieve a desired level of statistical power [@gigerenzer2004mindless].

## Aim of this thesis

As shown in this chapter, improving inferential methodology and addressing the replication crisis requires a broad set of interventions. One intervention is strengthening inference by applying the Neyman-Pearson framework as originally intended. Doing so, this thesis directly addresses two critical aspects of robust inference, often neglected in recent times: rigorous sample size planning and meaningful effect sizes interpretation.

To do this effectively, it is essential to pre-specify and formalize both the null and alternative hypotheses when designing a statistical test (as we outlined, when one is not able to formalize both hypothesis, more exploratory research should be done.) 
This approach avoids some of the major limitations of Null Hypothesis Significance Testing (NHST). As discussed, NHST does not support prospective power analysis, which is necessary to determine an adequate sample size. As a result, many studies are underpowered. Moreover, statistical significance is sometimes achieved for trivially small effects, leading to results that are only weakly informative. Finally, researchers frequently misinterpret p-values as indicators of the strength of an effect.

As we have shown, many of these issues can be addressed by explicitly considering the effect size under the alternative hypothesis. This allows for proper planning of adequately powered studies and requires a theoretical estimation of the expected effect, thereby emphasizing the importance of effect sizes. 

This thesis addresses the central challenge of specifying a meaningful alternative hypothesis by proposing a possible solution to facilitate its pre-specification.

In chapter two, we will demonstrate that interpreting effect sizes is crucial for obtaining meaningful results, although they are often assessed using uninformative benchmarks. We will introduce two approaches to ensure more meaningful consideration of effect sizes. The first is the "plausible effect size" [@gelman2014beyond], which helps define an effect size based on what is realistically expected in the context of the specific test. The second is the "Smallest Effect Size of Interest" [@anvari2021using], also known as SESOI, a concept recently applied in psychology that can help identify practically relevant effects. Finally, we will propose a method to compare these two approaches, assessing whether the effect sizes we consider meaningful are likely to be detected in practice.

Having established the foundation of this thesis, in chapter three we will explore expert elicitation procedures, methodologies for deriving effect sizes based on expert judgment, which we will use to determine the SESOI for the present research.

Finally, in chapter four, we bridge theory and practice by implementing the proposed framework. We demonstrate its practical utility by applying the integrated results to inform the design and power analysis of a preregistered study, which serves as an illustrative case study.
