{
  "hash": "7cac905e807266eb3ca7f6241862821a",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Enhance effect size interpretation \n\n\n<div style=\"text-align: right;\">\n*\"Thinking hard about effect sizes is imoprtant for any school of statistical inferences [i.e., Frequentist of Bayesian], but sadly a process often neglected.\"*\n</div>\n\n<div style=\"text-align: right;\">\n— Dienes, 2008, p.92\n</div>\n\nIn this chapter we will examine the effect size, providing a definition and a brief explanatory example.We will then outline its interpretation challenges, adressing the limits of statistical significance compared to the concept of practical significance. The distinction will alloww  us  to introduces methods for formulating meaningful hypotheses, therefore explaining the plausible effect size and the smallest effect size of interest (SESOI) to enhance inferential robustness. Then we will propose a practical methodology as a combination of the two above.\n\n## Why we need effect sizes\n\nIn the previous chapter, we highlighted the importance of formalizing an alternative hypothesis in hypothesis testing within the Neyman-Pearson framework, or, in other words, the importance of specifying an effect size for the study. As outlined, this would improve research in the social sciences in several ways. Firstly, by allowing us to condutct a proper design analysis, it would enable transparent preplanning of an appropriate sample size, thus reducing the risk of false positives [@altoe2020enhancing]. Secondly, the ability to predict effects and then observe whether they are confirmed or disconfirmed would lead to more falsifiable studies [@anvari2021using]. Furthermore, since *p*-values alone can lead to the acceptance of results with low practical significance, incorporating effect sizes reduces the risk of attributing importance to potentially meaningless findings. \nThese advantages highilight why effect sizes are essential complements to significance testing in study design and in result interpretation.\n\n## Effect size\n\nAs Peck and Flora report (2018, p.209), \"the term _effect size_ literally translates to some magnitude (or size) of the impact (or the effect) of a predictor or an outcome variable.\" so as to indicate a given amount one is interested in. As the definition depends on the  phenomenon of interest, there is a very wide amount of possible definition to determine an effect size [@kelley2012effect]. In psychology research, thought, effect sizes are most often used to indicate a relationship or a difference between two variables [@borenstein2021introduction]. Before detailing types, it is important to clarify the categories commonly discussed in the literature.\n\nThere are two main types of effect sizes reported in literature. Firstly, the raw mean difference,where the effect size is expressed in the same units as the original measurement scale [@borenstein2021introduction]. For example, it could be the mean difference between two groups on blood pressure measurement. Raw effect sizes offer a direct and intuitive idea of the meaning of the effect, especially given a widespread unit scale (as mmHg for the given example)[@borenstein2021introduction]. Nevertheless, many researches in social sciences use a variety of different measurements, adotping different scales [@altoe2020enhancing]. \n\nGiven this limitation, researchers often prefer standardized metrics to facilitate comparison across studies and contexts. There exist many standardised effect sizes. In addressing this matter in the present thesis, however, we will focus on the most widely used and well-known measure [@altoe2020enhancing], namely Cohen's *d* (δ) [@cohen1988statistical]. In a concise formulation, Altoè (2020) describes Cohen’s *d* as \"the row difference between two popoluation means ($\\mu$~A~ $\\mu$~B~) devided by the common standard deviation ($\\sigma$):\n\n$$\n\\delta =\\frac{\\mu_A - \\mu_B}{\\sigma}\n$$\nThis formulation allows for standard interpretation across different measurement scales, which enhances comparability in psychological research.\n\n### An example of effect size\n\nDue distribuzioni relative alla statistica test utilizzata per confrontare i punteggi medi al questionario K-10, pre e post intervento.\n\nLa linea rossa la distribuzione della differenza media dei punteggi sotto l’ipotesi nulla (H₀), secondo cui non vi è alcun effetto dell’intervento (ovvero la differenza attesa tra i gruppi è pari a zero). La seconda curva, centrata in 0.25, rappresenta invece la distribuzione della statistica test sotto l’ipotesi alternativa (H₁), assumendo che l’intervento abbia un effetto vero pari a un effect size standardizzato di 0.25.\n\nAs illustrated in figure 2.1\n\n\n\n\n\n\n## Interpretation of effect sizes\n\nWhen addressing effect sizes, it is important to distinguish between informative and uninformative ones. While this may not be necessary in all sciences, it goes without saying that in psychology we must be able to differentiate the magnitude of effects. Taking into account the clinical context, one in which this distinction is especially critical, it is clear that we need to discriminate between mild and highly effective interventions, just as we distinguish between severe and modest clinical cases.\n\nHowever, in psychology, there is a tendency to treat any effect size as a success [@hilgard2021maximal], which hinders a clear distinction between these two categories. To counter this issue, tt is usefull in this regard introducing the distintcion between practical and statistical significance, which are not alternatives, but complementary dimensions of result interpretation. \n\nStatistical significance is met when a certain *p*-value indicates that the result meets the level of evidence required by the researcher's chosen threshold to reject the null hypothesis (H~0~), but *p*-value alone provides no information about the magnitude or practical relevance of the observed effect [@kelley2012effect; @pek2018reporting]. A statistically significant result may correspond to a trivially small effect, especially in studies with large sample sizes, while relevant effects may go undetected in underpowered studies. \nPractical significance, instead, refers to the distinction between intresting and unintresting effects [@anvari2021using]. In fact, effect sizes themselves are not automatically informative of anything, and should be interpreted in light of substantive criteria [@grosson2012; @kim2012;@primbs2023].\n\nThese points illustrate the necessity of evaluating effect sizes within the specific context and goals of research. Despite this, the interpretation of effect sizes is usually related to a classification proposed by Cohen [@cohen1988statistical], who suggested values of d = 0.2, d = 0.5, d = 0.8 as indicative of \"small\", \"medium\" and \"large\" effect sizes respectively. \n\n::: {#imag_1 .center}\n![](../figures/02es.jpg){width=60%}\n\nFigure 2.2.1: A graphical rapresentation of 0.2 Choen's *d* [@magnussonCausalTherapistEffects]\n:::\n\n::: {#imag_1 .center}\n![](../figures/05es.jpg){width=60%}\n\nFigure 2.2.2: A graphical rapresentation of 0.5 Choen's *d* [@magnussonCausalTherapistEffects]\n:::\n\n::: {#imag_1 .center}\n![](../figures/08es.jpg){width=60%}\n\nFigure 2.2.3: A graphical rapresentation of 0.8 Choen's *d* [@magnussonCausalTherapistEffects]\n:::\n\nWhile Cohen himself intended these benchmarks as a last resource reference, psychology research has adopted them as standards.\nUnfortunately, applying these benchmarks without considering the context leads to uninformative conclusions [@funder2019evaluating]. Effect sizes require different interpretations depending on the field of interest, the specific content, and the research methods used, as in a given field a small effect size can be extremely relevant in practice[@altoe2020enhancing].\n\nThis clarification has motivated the development of more appropriate benchmarks for effect sizes. To establish such benchmarks, several methodologies have been proposed. Two of the main approaches are defining a \"plausible effect size\" and identifying the \"smallest effect size of interest\" (SESOI), both of which will be outlined below.\n\n### The plausible effect size \n\n\"The plausible effect size refers to what could be approximately the true value of the parameter in the population\"\nThis concept was introduced to address the challenges of power analysis—or more broadly, design analysis [@gelman2014beyond], by helping researchers pre-specify an alternative hypothesis, thereby enhancing inferential strength for the reasons previously discussed.\n\nAlthough the validity of the inferetnial process is generally strenghtened when hypotheses are derived from theory and formalized in statistical terms, precise hypotheses are often not yet feasible in psychology for the reasons previously discussed. Therefore, their formalization can instead be based on literature reviews and/or meta-analyses [@altoe2020enhancing].\n\nThe main advantage of plausible effect sizes lies in enabling a critical interpretation of the results obtained from the study, and to avoid adjusting effect sizes, either implicitly or explicitly, to justify the value of a given sample size, as it's often done [@gelman2014beyond]. As stated by Altoè  \"in general when observed effeect size falls outside the pre-specified plausible interval, we can conclude that the observed study is not coherent with our theoretical expectations. On the other hand, we could aslo consider that our plausible interval may be unrealistic and/or poorly formialized.\" (2020, p.9) \n\nAlthough this metodology is useful to establish criteria that make us cautious about implausible effects, its direct impact on the inferential process is limited, as they do not directly provide with guidelines to distinguish which effect sizes should be deemed meaningful within a given line of research. In exploring further alternative solutions to this issue, the second aforementioned approach will now be explained.\n\n### The smallest effect size of interest\n\nThe firs relevant ideas on this matter have emerged from the medical field, where quantifying the effect of a given phenomenon is clearly essential. These methods aim to quantify a _Minimal Important Difference_ (MID). Subspecifications of this concept include the _Minimally Detectable Difference_ (MDD), which emphasizes the ease of detecting a given difference, and the _Clinically Important Difference_ (CID), which is based on relevant clinical outcomes such as recurrence or risk of rehospitalization [@norman2003interpretation]. \n\nOne similiar concept has been recently proposed in psychology, the aforementioned *Smallest Effect Size Of Interest* (SESOI). The SESOI can be defined as \"the smallest change that is needed in the outcome measure for people to subjectively notice and report a difference in how they feel\"  [@anvari2021using, p.1]. This concept translates the notion of an \"important\" change, defined as one that is both minimal and noticeable, into psychological research, providing a practical criterion for assessing the relevance of effect sizes.\n\nThe idea is to assess the importance of a given amount of change based on an external practical criterion. As with previous cases, such a criterion cannot yet rely on solid theoretical reasoning [@riesthuis2024statistical]. However, several methodologies have been proposed, though still being at an exploratory stage. The most prominent among these include cost-benefit analyses, anchor-based methods, and consensus methods [@anvari2021using].\n\n1) Cost–benefit analysis evaluates whether the observed improvement justifies the cost of the intervention, usually in comparison with alternatives. However, as noted by Anvari and Lakens themselves, “in basic psychology research, costs and benefit are not easily quantified” (2021, p.2).\n\n2) Anchor based methods use a retrospective judgment as a reference to determine whether participants have improved, stayed the same, or worsened over some period of time [@norman2003interpretation; @lydick1993interpretation; @anvari2021using]. For example to estimate how much change has been experienced after a treatment, participants complete the measurement of interest before and after the treatment. They are then asked to indicate how much change they notice. Responses may vary depending on the rating scale used. These ratings can be collected through self-reports or clinician’s assessments, and are colled clinical \"anchor\". Then, the amount of change (effect size) corresponding to each category is measured.[@anvari2021using]\n\n3) Consensus methods are a recently proposed solution that involve asking experts for their opinion on what could constitute the smallest effect size of interest. Researchers then assess whether there is general agreement on the SESOI [@anvari2021using; @riesthuis2022expert]. \n\nTogether, these methods provide preliminary yet important frameworks for defining practically meaningful effect sizes in psychological research.\n\n## A methodology for building a plasuible SESOI\n\nBased on what has been explained so far, it should be clear how a well-established and pre-specified effect size can strengthen inferential process and enhance the overall credibility of psychological research. In particular, the concept of a plausible effect size provides a solid basis for defining reasonable intervals for the true effect of a phenomenon. Conversely, methods for determining a Smallest Effect Size of Interest (SESOI) offer valuable guidance for identifying effects that are practically meaningful.\n\nBuilding on this foundation, the aim of this thesis is to propose a methodology that uses both concepts in a complementary way. Specifically, we seek to assess whether the SESOI identified is contained within the plausible interval for the effect size or, conversely, whether the effect sizes considered plausible include effects that are practically meaningful—thus ensuring that the estimated true effect is not only statistically valid but also relevant.\n\nThe procedure goes as follows: \n\n1) Since systematic reviews and meta-analyses can offer proper guidance on typical effect sizes [@gelman2014beyond], a plausible effect size will be determined through meta-analytic methods.\n2) As consensus methods are appropriate for guiding decisions in setting a SESOI that reflects practical relevance [@riesthuis2024statistical], this approach will be used. Particular attention on this matter will be given in the following chapter.\n\nThe figure 2.2 shows how these two measures could be hypothetically be rapresentated.\n\n::: {#imag_1 .center}\n![](../figures/img2.png){width=80%}\n\nFigure 2.2: Obtaining a Plausible effect size and a SESOI\n:::\n\n3) The two result will be than compared, as illlustrated in figure 2.3 and discussed. \n\nAs illustrated in figure 2.3\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](../figures/img3.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nFigure 2.3: Matching SESOI and the Plausible effect size\n\n\n4) Finally, a design analysis will be conducted on the obtained result, to illustrate the practical utility of the proposed methodology.  \n\nAs consensus methods have only recently and infrequently been used to assess a SESOI, the following chapter will focus on these approaches, and more specifically, on their more systematic form: expert elicitation. \n\n\n## Briefly \n\n\n\n----------------> richiama quanto scritto nel paragrafo precedente. quindi il fatto che spesos le persone usano il pvalue come misura dell’effetto + che che entro il NHST le persone non si filano il sample size e la power, mentre l’effect size è connesso con entrambe queste cose. \n\n\n\n\n\n\n\n",
    "supporting": [
      "chapter2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}