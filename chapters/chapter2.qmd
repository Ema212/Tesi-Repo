
# Enhance effect size interpretation 

\begin{flushright}
\textit{"Thinking hard about effect sizes is important for any school of statistical inferences [i.e., Frequentist or Bayesian], but sadly a process often neglected."}

— Dienes, 2008, p.92
\end{flushright}

::: {.empty}
\vspace{5mm}
:::

In this chapter we will examine the effect size, providing a definition and a brief explanatory example. We will then outline its interpretation challenges, adressing the limits of statistical significance compared to the concept of practical significance. The distinction will allow  us  to introduce methods for formulating meaningful hypotheses, therefore explaining the plausible effect size and the smallest effect size of interest (*SESOI*) to enhance inferential robustness. Then we will propose a practical methodology as a combination of the two above.

## Why we need effect sizes

In the previous chapter, we highlighted the importance of formalizing an alternative hypothesis while doing hypothesis testing within the Neyman-Pearson framework, or, in other words, the importance of specifying an effect size for the study. As outlined, this would improve research in the social sciences in several ways. Firstly, by allowing us to conduct a proper design analysis, it would enable transparent preplanning of an appropriate sample size, thus reducing the risk of false positives [@altoe2020enhancing]. Secondly, the ability to predict effects and then observe whether they are confirmed or disconfirmed would lead to more falsifiable studies [@anvari2021using]. Furthermore, since *p*-values alone can lead to the acceptance of results with low practical significance, incorporating effect sizes reduces the risk of attributing importance to potentially meaningless findings. 
These advantages highlight why effect sizes are essential complements to significance testing in study design and in result interpretation.

## Effect size

As Pek and Flora report (2018, p.209), "the term _effect size_ literally translates to some magnitude (or size) of the impact (or the effect) of a predictor or an outcome variable". Briefly said, it quantifies a given effect. As the definition depends on the  phenomenon of interest, there is a very wide amount of possible definition to determine an effect size [@kelley2012effect]. In psychology research, though, effect sizes are most often used to indicate a relationship or a difference between two variables [@borenstein2021introduction]. 

There are two main ways to report effect sizes in the literature. Firstly, the raw mean difference,where the effect size is expressed in the same units as the original measurement scale [@borenstein2021introduction]. For example, the raw mean difference could quantify the mean difference between two groups on blood pressure measurement. Raw effect sizes offer a direct and intuitive idea of the meaning of the effect, especially given a widespread unit scale (as mmHg for the given example)[@borenstein2021introduction]. Nevertheless, many researches in social sciences use a variety of different measurements, adotping different scales, making it challenging to have a shared understanding of raw effect sizes [@altoe2020enhancing]. 

Given this limitation, researchers often prefer a second approach, i.e. standardized effect sizes, which facilitate comparison across studies and contexts. For this purpose, many different standardized effect sizes exist. In addressing this matter in the present thesis, however, we will focus on the most widely used and well-known measure [@altoe2020enhancing], namely Cohen's *d* ($\mu$~d~) [@cohen1988statistical]. In a concise formulation,  Cohen’s *d* is the raw difference between two popoluation means ($\mu$~A~ $\mu$~B~) devided by the common standard deviation ($\sigma$):

$$
\delta =\frac{\mu_A - \mu_B}{\sigma}
$$
This formulation allows for standard interpretation across different measurement scales, which enhances comparability in psychological research.

To illustrate how a standardized effect size such as Cohen’s *d* operates in practice, we provide a practical illustration in the following section.

### Illustrating effect size

To illustrate how a standardized effect size such as Cohen’s d operates in practice, we now present a simplified example based on a pre-post intervention study in a clinical context. In such studies, psychological distress is typically measured before and after an intervention using standardized instruments, for example, the Kessler Psychological Distress Scale (K-10). Participants are randomly assigned to a treatment group or a control group, and the effect of the intervention is assessed by comparing changes in K-10 scores. 
In the following example, intended solely for didactic purposes, we provide a visual and conceptual representation of what it means to specify an effect size under the null and alternative hypotheses. Specifically, our alternative hypothesis specifies an expected standardized effect of *d* = 0.41.

The figure 2.1 shows two theoretical distributions of the test statistic under the null hypotheses (H~0~) and alternative hypotheses (H~1~), within a Neyman-Pearson framework. These distributions derive from a comparison between a treatment group and a control group in a pre-post design. The black curve represents the distribution under the null hypothesis (H~0~), which assumes that the intervention has no effect (i.e., the mean change in the treatment group is equal to that in the control group) corresponding to an effect size of *d* = 0. The blue curve represents the alternative hypothesis (H~1~), assuming a true standardized effect size of *d* = 0.41, meaning the treatment group improves, on average, by 0.41 standard deviations more than the control group. The distance between the two distributions reflects the assumed effect magnitude. 

\vspace{0.5cm}
![A graphical rapresentation of a Cohen's *d*, in a RCT group (Magnusson, 2023).](../figures/esgen.png){width=80% fig-align=center}

We have provided a graphic illustration of an effect size, which represents values distribution when the treatment has no effect (H~0~) and when the effect is 0.41 (H~1~). It is quite intuitive though that neither a graphical representation nor a numerical value clearly informs us of _how much_ of improvement patient have experienced under the alternative hypothesis [@funder2020evaluating]. This example introduces the main matter discussed in this thesis that we will detail in the following section, that is interpretation of effect size.

## Interpretation of effect sizes

When addressing effect sizes, it is important to distinguish between informative and uninformative ones. While this may not be necessary in all sciences, it goes without saying that in psychology we must be able to differentiate the magnitude of effects. Taking into account the clinical context, one in which this distinction is especially critical, it is clear that we need to discriminate between mild and highly effective interventions, just as we distinguish between severe and modest clinical cases.

However, in psychology, since there is no guidance for interpreting effect sizes [@ferguson2016effect], there is a tendency to treat any effect size as a success [@hilgard2021maximal; @anvari2023not], which hinders a clear distinction between these two categories. To counter this issue, it is useful in this regard introducing the distinction between practical and statistical significance, which are not alternatives, but complementary dimensions of result interpretation. 

### Statistical significance and practical significance

Statistical significance is met when a certain *p*-value indicates that the result meets the level of evidence required by the researcher's chosen threshold ($\alpha$) to reject the null hypothesis (H~0~), but *p*-value alone provides no information about the magnitude or practical relevance of the observed effect [@kelley2012effect; @pek2018reporting; @boring1919mathematical]. A statistically significant result may correspond to a trivially small effect, especially in studies with large sample sizes, while relevant effects may go undetected in underpowered studies. 
Practical significance, instead, refers to the distinction between interesting and uninteresting effects [@anvari2021using]. In fact, as already shown, effect sizes themselves are not automatically informative of anything, and should be interpreted in light of substantive criteria [@grissom2012effect; @primbs2023].

### The misuse of Cohen's classification benchmarks

These points illustrate the necessity of evaluating effect sizes within the specific context and goals of research. Despite this, the interpretation of effect sizes is usually related to a classification proposed by Cohen [@cohen1988statistical], who suggested values of *d* = 0.2, *d* = 0.5, *d* = 0.8 as indicative of "small", "medium" and "large" effect sizes respectively. 

A graphical representation of the three effect sizes is reported at figure 2.2, 2.3 and 2.4 respectively. These sets of distributions derive from a hypothetical comparison between a treatment group and a control group in a pre-post design. The black curves always represent the distribution under the null hypothesis (H~0~), which assumes that the intervention has no effect (i.e., the mean change in the treatment group is equal to that in the control group) corresponding to an effect size of *d* = 0. The blue curves always  represent the alternative hypothesis (H~1~), assuming a true standardized effect size of of *d* = 0.2, *d* = 0.5, *d* = 0.8, meaning the treatment group improves, on average, by 0.2, 0.5 and 0.8 standard deviations more than the control group, respectively. The distance between each pair of distributions reflects the assumed effect magnitudes. 

![A graphical representation of 0.2 Cohen's *d* (Magnusson, 2023)](../figures/02es.jpg){width=70% fig-align=center}

![A graphical representation of 0.5 Cohen's *d* (Magnusson, 2023)](../figures/05es.jpg){width=70% fig-align=center}

![A graphical representation of 0.8 Cohen's *d* (Magnusson, 2023)](../figures/08es.jpg){width=70% fig-align=center}

While Cohen himself intended these benchmarks as a last resource reference, psychology research has adopted them as standards. Unfortunately, applying these benchmarks without considering the context leads to uninformative conclusions [@funder2019evaluating; @lakens2016improving]. Effect sizes require different interpretations depending on the field of interest, the specific content, and the research methods used, as in a given field a small effect size can be extremely relevant in practice [@altoe2020enhancing].

## Alternative approaches to interpret effect sizes

This clarification has motivated the development of more appropriate benchmarks for effect sizes that are more context specific and informative. 

A first category of solution proposed by Funder and colleagues (2020), is comparing findings to well-established results in the literature, e.g. one could compare effect sizes typically considered "small" with classical results obtained in studies on attitude change [@festinger1959cognitive], or with to others non-psychology-related phenomena. However, by itself, this approach does not fully resolve the issue. Even if it could be useful to know whether results are comparable to those of another study, if there is no concrete or meaningful sense of the effect in the previous study, we know merely that the effects are similar, but nothing about the meaning they convey. The implications are even less clear when applied across different fields. Therefore, solutions relying solely on conventional or representative benchmarks are unlikely to be sufficient.

To establish more voncingin benchmarks though, several methodologies have been proposed. Two of the main approaches are defining a "plausible effect size" [@gelman2014beyond] and identifying the "smallest effect size of interest" (*SESOI*) [@anvari2021using], both of which will be outlined below.

### The plausible effect size 

"The plausible effect size refers to what could be approximately the true value of the parameter in the population" [@altoe2020enhancing, p.5]
This concept was introduced to address the challenges of power analysis, or more broadly, design analysis [@gelman2014beyond], by helping researchers pre-specify an alternative hypothesis, thereby enhancing inferential strength for the reasons previously discussed.

Although the validity of the inferential process is generally strengthened when hypotheses are derived from theory and formalized in statistical terms, precise hypotheses are often not yet feasible in psychology for the reasons previously discussed. Therefore, their formalization can instead be based on literature reviews and/or meta-analyses [@altoe2020enhancing].

The main advantage of plausible effect sizes is to enable a critical interpretation of the results, effectively discriminating plausible from implausible results. With this methodology, we can avoid adjusting effect sizes, either implicitly or explicitly, to justify the value of a given sample size, as it's often done [@gelman2014beyond]. As stated by Altoè  "in general when observed effect size falls outside the pre-specified plausible interval, we can conclude that the observed study is not coherent with our theoretical expectations. On the other hand, we could aslo consider that our plausible interval may be unrealistic and/or poorly formalized." (2020, p.9) 

Although this metodology is useful to establish criteria that make us cautious about implausible effects, its direct impact on the inferential process is limited, as they do not directly provide with guidelines to distinguish which effect sizes should be deemed meaningful within a given line of research. In exploring further alternative solutions to this issue, the second aforementioned approach will now be explained.

### The smallest effect size of interest

In the context of clinical research, scientists are particularly interested in the practical significance of a certain quantity of change. The first relevant ideas on this matter have emerged from the medical field, where quantifying the effect of a given phenomenon is clearly essential. These methods aim to quantify a _Minimal Important Difference_ (MID). Subspecifications of this concept include the _Minimally Detectable Difference_ (MDD), which emphasizes the ease of detecting a given difference, and the _Clinically Important Difference_ (CID), which is based on relevant clinical outcomes such as recurrence or risk of rehospitalization [@norman2003interpretation]. 

One similiar concept has been recently proposed in psychology, the aforementioned *Smallest Effect Size Of Interest* (*SESOI*). The *SESOI* can be defined as "the smallest change that is needed in the outcome measure for people to subjectively notice and report a difference in how they feel"  [@anvari2021using, p.1]. This concept translates the notion of an "important" change, defined as one that is both minimal and noticeable, into psychological research, providing a practical criterion for assessing the relevance of effect sizes.

The idea is to assess the importance of a given amount of change based on an external practical criterion. As preivously discussed, such a criterion cannot yet rely on solid theoretical reasoning [@riesthuis2024statistical]. However, several methodologies have been proposed, though still being at an exploratory stage. The most prominent among these include cost-benefit analyses, anchor-based methods, and consensus methods [@anvari2021using].

1) Cost–benefit analysis evaluates whether the observed improvement justifies the cost of the intervention, usually in comparison with alternatives. However, as noted by Anvari and Lakens themselves, “in basic psychology research, costs and benefit are not easily quantified” (2021, p.2).

2) Anchor based methods use a retrospective judgment as a reference to determine whether participants have improved, stayed the same, or worsened over some period of time [@norman2003interpretation; @lydick1993interpretation; @anvari2021using]. For example to estimate how much change has been experienced after a treatment, participants complete the measurement of interest before and after the treatment. They are then asked to indicate how much change they notice. Responses may vary depending on the rating scale used. These ratings can be collected through self-reports or clinician’s assessments, and are called clinical "anchor". Then, the amount of change (effect size) corresponding to each category is measured [@anvari2021using]. However, these methods are quiet recent, and not very much established. 

3) Consensus methods are a recently proposed solution that involve asking experts for their opinion on what could constitute the smallest effect size of interest. Researchers then assess whether there is general agreement on the *SESOI* [@anvari2021using; @riesthuis2022expert]. 

Together, these methods provide preliminary yet important frameworks for defining practically meaningful effect sizes in psychological research.

## A methodology for building a plausible *SESOI*

Based on what has been explained so far, it should be clear how a well-established and pre-specified effect size can strengthen inferential process and enhance the overall credibility of psychological research. In particular, the concept of a plausible effect size provides a solid basis for defining reasonable intervals for the true effect of a phenomenon. Conversely, methods for determining a Smallest Effect Size of Interest (*SESOI*) offer valuable guidance for identifying effects that are practically meaningful.

In table 2.1 we provide a concise summary of the advantages and disadvantages of both approaches.

|               | **Plausible effect size**   | **SESOI**                          |
|---------------|-----------------------------|------------------------------------|
| **Pros**      | • Evidence based            | • Practically informative          |
| **Cons**      | • Practically uninformative | • Strongly subjective              |
| **Require**   | • High-quality meta-analysis| • Attentive elicitation procedures |

Table: A synthetic comparison of the *SESOI* and the plausible effect size.

\vspace{0.2\baselineskip}

Building on this foundation, we propose a methodology that uses both concepts in a complementary way, leveraging their respective advantages. Specifically, we seek to assess whether the *SESOI* identified is contained within the plausible interval for the effect size or, conversely, whether the effect sizes considered plausible include effects that are practically meaningful, thus ensuring that the estimated true effect is not only statistically valid but also relevant.

The procedure goes as follows: 

1) Since systematic reviews and meta-analyses can provide guidance on typical effect sizes [@gelman2014beyond], we can derive a plausible effect size using meta-analytic data. For illustration purpose, we assumed hypothetical meta-analytic results, evaluating efficacy of a clinical intervention, measured with a questionnaire that asses well-being levels. In this scenario, the meta-analysis indicates that the Coehn's *d* compared to the control was 0.44, with a 95% confidence interval from 0.27 to 0.61, indicating an improvement in well-being levels. We decide to set our plausible effect size within the confidence interval. A graphical hypothetical representation is shown in figure 2.5.

\vspace{0.5cm}
![A graphical rapresentation of a hypothetical plausible effect size.](../figures/plaus.png){width=80% fig-align=center}

2) As consensus methods are appropriate for guiding decisions in setting a *SESOI* that reflects practical relevance [@riesthuis2024statistical], we will use  this approach to generate one. We will give particular attention to the matter of consensus methods in the following chapter. For clarity, we generated three different possible *SESOI* scores that may emerge from three different elicitation processes, as graphically represented in figure 2.6.

\vspace{0.5cm}
![A graphical rapresentation of the hypothethical *SESOI*s.](../figures/3effs.png){width=80% fig-align=center}

3) The two results are then compared, as illustrated in figure 2.7 and discussed. The matter of comparison requires critical and deep thinking, depending heavily on the choices about how both the *SESOI* and the plausible effect size are collected (we will provide a concrete example of such choices and discuss the direct implementation of this framework in chapter 4.) Following the comparison, results  are interpreted. In the case of the first and smallest generated *SESOI* score, for example, we might think that since the plausible effect size falls after the line of clinical significance, the vast majority of results in this particular field of research are clinically relevant. With the second and middle-ranged *SESOI* score, we could say that most of the results in this field are somewhat clinically relevant. With the last and largest *SESOI* score, we might argue that our clinical interventions are clinically irrelevant, even though the effect size might sometimes be even medium or large, based on Cohen's standards. Conversly, we could also say that our questionnaire are not suited to effectively identify a clinically relevant change. Obviously, results are never so easily discussed, and deeper reasoning is yet to be done.  

\vspace{0.5cm}
![The comparison between the plausile effect size and the *SESOI*s.](../figures/graphmatch.png){width=80% fig-align=center}

4) Finally, a design analysis will be conducted on the obtained result, to illustrate the practical utility of the proposed methodology.  

As consensus methods have only recently and infrequently been used to assess a *SESOI*, the following chapter will focus on these approaches, and more specifically, on their more systematic form: expert elicitation. 

## Conclusions

In the previous chapter, we discussed how the *NHST* framework often leads researchers to overlook power, sample size, and the practical meaning of results, with p-values frequently misused as indicators of effect strength. Here, we focus on effect sizes as essential tools for addressing these issues, as they connect directly to both study planning and result interpretation.

We defined the effect size, outlined common interpretative pitfalls, and distinguished statistical from practical significance. To enhance interpretation, we presented two complementary approaches: the plausible effect size, based on prior evidence or theory, and the Smallest Effect Size of Interest (*SESOI*), based on practical relevance. We then proposed a method to integrate both, comparing whether plausible effects are also meaningful.

This chapter shows that careful consideration of effect sizes strengthens inference,  supporting the application of the Neyman-Pearson framework, and improves research utility. In the next chapter, we will explore expert elicitation as a method to define the *SESOI* within our framework.
