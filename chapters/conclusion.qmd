# Conclusions
Psychology is facing a theoretical and methodological crisis, as discussed in Chapter 1, which is primary cause for the replicability crisis, undermining the credibility of the discipline. We believe rigorous research practices and transparency are crucial to restore trust in the field. 

This thesis therefore aims strengthen inference by applying the Neyman-Pearson framework as originally intended. To do this, effect size pre-specification is deemed crucial. One way to encourage effect size pre-specification is try to develop meaningful interpretation for effect sizes. In order to do so, we proposed and applied a methodology for integrating plausible effect sizes with the smallest effect size of interest (SESOI). The plauisible effect size gives the best synthesis literature has achieved about effect sizes on a certain field, while we believe that expert elicitation offers a crucial, clinically-grounded interpretation of what constitutes a meaningful change. Combining these two pieces of information highly contribute to help pre-specify effect sizes, as it gives a solid line of interpretations for effect sizes, therefore encouraging the adoption of rigorous research standards. 

In this concluding section we will synthesize the contributions, acknowledge the limitations, and outline the prospective trajectories of the developed framework.

## General results 
The primary contribution of this thesis lies not in a specific empirical finding, but in the development and practical illustration of the procedural framework for enhancing the interpretation and planning of psychological studies. The ultimate aim is to provide a replicable template for researchers to use.

Having addressed the inadequacies in the quality of psychological theories and issues related to the interpretation of effect sizes and the proliferation of under-powered studies, we propose a comprehensive framework designed to guide researchers in conceptualizing and pre-specifing effect sizes, and taking accurate choices in conducting power analysis before the study. This framework systematically combines information from both the existing literature and clinical expertise, creating a more robust foundation for study design.

The challenges we encountered largely reflect the pioneering stage of applying SESOI in a clinical context. By documenting the initial exploration of this methodology along with its obstacles, we hope to offer a preliminary step for future research to develop more refined and effective approaches.

## Limitations
While the proposed framework offers a substantial potential, we also acknowledge its limitations, requiring further refinement. Thhe methodological challenges we faced are largely inherent to the novelty of expert elicitation procedures in this context. Rather than strict limitations, these points may be considered constructive pathways for future development, which we will elaborate upon next.

The limitations can be grouped in two main cathegories:

- **Methodological limitations of the tailored elicitation approach:**  These constraints, primarily concerning the informal nature of our procedure, could be addressed by an established manualized elicitation method or developing a standardized protocol *ad hoc*.
- **Limitations related to the research context:** these limitations derive from a scarcity of necessary data in the existing literature, which constrained the validity of our results. This highlights the urgent need for new primary research to address these gaps and promote open data sharing.

### Elicitation related
- A key challenge was the inherent circularity in defining the "minimally important change". To anchor this abstract concept, we used a clinical vignette depicting a patient with emerging depressive symptoms, asking experts to envision a slight but meaningful functional improvement. We then asked the experts to quantify that improvement into the expected Kessler-10 score change. While this provided a concrete reference, the procedure remained intrinsically subjective, relying on individual clinicians' interpretations of the central construct, potentially introducing more variability. This underscores the importance of eliciting multiple expert judgments to capture diverse clinical perspectives. By eliciting only point estimates rather than uncertainty intervals, this approach does not capture their confidence levels and may reinforce overconfidence bias. This limitation could be mitigated in future research by employing a formal elicitation protocol, such as the IDEA protocol, which includes dedicated phases for properly introducing and aligning experts' understanding of the target constructs.

- Some experts mistook the SESOI with a measure of treatment efficacy. However, the SESOI defines a clinically meaningful score change on the K-10, regardless of how that improvement is achieved (e.g., treatment, spontaneous remission, or placebo effect). This conceptual confusion was evident when experts inquired about the specific treatment, revealing a focus on the source of improvement rather than the score change that defines it. This underscores a current knowledge gap between methodology and practice. As concepts like SESOI become more integrated into clinical training, this barrier will diminish, facilitating future applications of these techniques.

### Research context related
- The heterogeneity of outcome measures across meta-analyses limited the comparability between the meta-analytic effect size and the elicited SESOI. A synthesis based on a single instrument would have been preferable, but such standardization remains rare in the literature.

- Since the literature on treatment efficacy mostly reports effect sizes between-groups using Cohen's *d*, a directly comparable approach would be to elicit a score change reflecting improvement relative to a control group. We believe, however, that this method may be conceptually problematic for two reasons. Clinicians lack direct experience with control groups, which could undermine the very expertise elicitation seeks to capture. Furthermore, this framing implicitly assumes that the control group cannot show clinically relevant effects, a significant assumption that must be discussed. While this limits direct comparability, and could be further discussed, our chosen approach prioritizes the ecological validity of the clinical judgment we sought to elicit.

## Future develompents
Since our primary goal was to introduce a novel framework, we opted for simplicity as the most effective
explanatory approach. Future work, however, could significantly extend this methodology by building more advanced elicitation protocols, collecting more comprehensive data, or even conducting a dedicated meta-analysis tailored for this purpose.

Particularly, building on our experience, we believe that important future developments can be structured around the two main categories of limitations we encountered:

1) **Refining the Elicitation Methodology**: future studies should move beyond our tailored approach by implementing a formal, manualized elicitation protocol, such as the full IDEA framework, or developing a more appropriate variation, to address the issue of the introduction of the SESOI concept, also mitigating the conceptual confusion we observed between a meaningful score change and a specific treatment effect. Following, these approaches could elicit probability distributions or credible intervals instead of single point estimates. As precedently outlined, this would capture expert uncertainty, reduce overconfidence bias, and provide a more solid basis for power analysis.

2) **Enhancing the Research Context and Integration**: given the constraints of current literature, future studies could conduct a meta-analysis focused on a single, clinically key outcome measure (like the K-10), already adapted for thw within metric. This would directly solve the problem of heterogeneous metrics and provide more precise, comparable estimates of the plausible effect size. Lastly, future research could test this framework by designing new studies where SESOI elicitation guides the planning from the very beginning. This would make it possible to collect all the necessary within-subject, avoinding the need of relying on large statistical assumptions and approximations.

It is our hope that this framework can serve as a catalyst for adopting more rigorous research practices. We aspire for it foster critical thinking around effect size and statistical power, especially in clinical psychology, where theoretical, methodological and ultimately clinical decisions have a direct impact on people's lives, a profound responsability that can not be dismissed superficially.