# Conclusions
Psychology is facing a theoretical and methodological crisis, as discussed in Chapter 1, which is a primary cause for the replicability crisis, undermining the credibility of the discipline. We believe rigorous research practices and transparency are crucial to restore trust in the field. 

This thesis therefore aims to strengthen inference moving within the Neyman-Pearson framework. To do this, effect size pre-specification is deemed crucial. One way to encourage effect size pre-specification is to try to develop meaningful interpretation for effect sizes. In order to do so, we proposed and applied a methodology for integrating plausible effect sizes with the smallest effect size of interest (SESOI) [@anvari2021using]. The plausible effect size [@altoe2020enhancing] gives the best synthesis literature has achieved about effect sizes on a certain field, while we believe that expert elicitation offers a crucial, clinically grounded interpretation of what constitutes a meaningful change. Combining these two pieces of information greatly helps pre-specifying effect sizes, as it gives a solid line of interpretation for effect sizes, therefore encouraging the adoption of rigorous research standards. 

In this concluding section we will synthesize the contributions, acknowledge the limitations, and outline the prospective trajectories of the developed framework.

## General results 
The primary contribution of this thesis lies not in a specific empirical finding, but in the development and practical illustration of the procedural framework for enhancing the interpretation and planning of psychological studies. The ultimate aim is to provide a useful methodology for researchers to use and apply in different contexts.

Having addressed the inadequacies in the quality of psychological theories and issues related to the interpretation of effect sizes and the proliferation of under-powered studies, we propose a comprehensive framework designed to guide researchers in conceptualizing and pre-specifying effect sizes, and taking accurate choices in conducting power analysis before the study. This framework systematically combines information from both the existing literature and clinical expertise, creating a more robust foundation for study design.

The applied methodology informed us that a target effect size around *d* â‰ˆ 0.55 should be both clinically relevant and empirically plausible. Building on this information we conducted a power analysis for a range of plausible values around the target effect size.

The sensitivity analysis shows the trade-offs involved in study planning (table 5.1), providing a clear, quantitative basis for resource allocation and study design decisions, moving beyond convention or guesswork.

| Effect Size | Power 70%   | Power 80%   | Power 90%   |
|-------------|-------------|-------------|-------------|
| d = 0.50    | 51 (102)    | 64 (128)    | 86 (172)    |
| d = 0.55    | 42 (84)     | 53 (106)    | 71 (142)    |
| d = 0.60    | 36 (72)     | 45 (90)     | 60 (120)    |

Table: Participants per group across effect sizes and power levels:

As discussed in chapter 4, as a practical illustration, a researcher adopting a standard power threshold of 80% and a conservative target effect size of *d* = 0.50, justified by our methodology as both clinically meaningful and empirically plausible, would require a total sample size of 128 participants.

The challenges we encountered largely reflect the pioneering stage of applying SESOI in a clinical context. By documenting the initial exploration of this methodology along with its obstacles, we hope to offer a preliminary step for future research to develop more refined and effective approaches.

## Limitations
While the proposed framework offers a substantial potential, we also acknowledge its limitations, requiring further refinement. The methodological challenges we faced are largely inherent to the novelty of expert elicitation procedures in this context. Rather than strict limitations, these points may be considered constructive pathways for future development, which we will elaborate upon next.

The limitations can be grouped in three main categories:

- **Methodological limitations of the tailored elicitation approach:**  These constraints, primarily concerning the informal nature of our procedure, could be addressed by an established manualized elicitation method or developing a standardized protocol *ad hoc*.
- **Limitations related to the research context:** these limitations derive from a scarcity of necessary data in the existing literature, which constrained the validity of our results. This highlights the urgent need for new primary research to address these gaps and promote open data sharing.
- **Limitations related to statistical assumption:** these constraints were proved necessary in order to permit the comparison. While reasonable, they underscore the need to address the limitations mentioned earlier.

### Elicitation related
A key challenge was the inherent circularity in defining the "minimally important change". To anchor this abstract concept, we used a clinical vignette depicting a patient with emerging depressive symptoms, asking experts to envision a slight but meaningful functional improvement. We then asked the experts to quantify that improvement into the expected Kessler-10 score change. While this provided a concrete reference, the procedure remained intrinsically subjective, relying on individual clinicians' interpretations of the central construct, potentially introducing more variability. This underscores the importance of eliciting multiple expert judgments to capture diverse clinical perspectives. By eliciting only point estimates rather than uncertainty intervals, this approach does not capture their confidence levels and may reinforce overconfidence bias. This limitation could be mitigated in future research by employing a formal elicitation protocol, such as the IDEA protocol [@hemming2018practical], which includes dedicated phases for properly introducing and aligning experts' understanding of the target constructs. We faced an additional complication when some experts mistook the SESOI with a measure of treatment efficacy. However, the SESOI defines a clinically meaningful score change on the K-10, regardless of how that improvement is achieved (e.g., treatment, spontaneous remission, or placebo effect). This conceptual confusion was evident when experts inquired about the specific treatment, revealing a focus on the source of improvement rather than the score change that defines it. This underscores a current knowledge gap between methodology and practice. As concepts like SESOI become more integrated into clinical training, this barrier will diminish, facilitating future applications of these techniques.

### Research context related
One primary limitation was the heterogeneity of outcome measures across meta-analyses limited the comparability between the meta-analytic effect size and the elicited SESOI. A synthesis based on a single instrument would have been preferable, but such standardization remains rare in the literature. Additionally, regarding the elicitation question, since the literature on treatment efficacy mostly reports effect sizes between-groups using Cohen's *d*, a directly comparable approach would be to elicit a score change reflecting improvement relative to a control group. We believe, however, that this method may be conceptually problematic for two reasons. Firstly, clinicians lack direct experience with control groups, which could undermine the very expertise elicitation seeks to capture. Furthermore, this framing implicitly assumes that the control group cannot show clinically relevant effects, a significant assumption that must be discussed. While this limits direct comparability, and could be further discussed, our chosen approach prioritizes the ecological validity of the clinical judgment we sought to elicit.

### Statistical limitations 
As illustrated in previous chapter, the conversion between within-subject and between-groups effect sizes required several statistical assumptions that, while reasonable, represent limitations to our approach. Specifically, we assumed that:

1) Randomization ensures baseline equivalence between treatment and control group. This is a fundamental property of randomized designs and ensures that groups are comparable at baseline.
2) the control group shows negligible change over time, which is reasonable when using waitlist controls or minimal-attention control conditions that are not expected to produce therapeutic effects. 
3) the post-test SD is approximately equal to the baseline SD, an assumption commonly made in power analysis and sample size planning when prior information about variance changes is unavailable. 

## Future developments
Since our primary goal was to introduce a novel framework, we opted for simplicity as the most effective
explanatory approach. Future work, however, could significantly extend this methodology by building more advanced elicitation protocols, collecting more comprehensive data, or even conducting a dedicated meta-analysis tailored for this purpose.

Particularly, building on our experience, we believe that important future developments can be structured around the two main categories of limitations we encountered:

1) **Refining the Elicitation Methodology**: future studies should move beyond our tailored approach by implementing a formal, manualized elicitation protocol, such as the full IDEA framework, or developing a more appropriate variation, to address the issue of the introduction of the SESOI concept, also mitigating the conceptual confusion we observed between a meaningful score change and a specific treatment effect. Following, these approaches could elicit probability distributions or credible intervals instead of single point estimates. As previously outlined, this would capture expert uncertainty, reduce overconfidence bias, and provide a more solid basis for power analysis.

2) **Enhancing the Research Context and Integration**: given the constraints of current literature, future studies could conduct a meta-analysis focused on a single, clinically key outcome measure (like the K-10), already adapted for the within metric. This would directly solve the problem of heterogeneous metrics and provide more precise, comparable estimates of the plausible effect size. Lastly, future research could test this framework by designing new studies where SESOI elicitation guides the planning from the very beginning. This would make it possible to collect all the necessary within-subject, avoiding the need of relying on large statistical assumptions and approximations.

Once these current limitations have been adequately addressed, future research could focus on applying this methodology to different questionnaires and psychological constructs.

In conclusion, we hope this framework can encourage researchers to adopt more rigorous practices. Our goal is for it to inspire more critical thinking about effect size and statistical power especially in clinical psychology, where theoretical, methodological and ultimately clinical decisions have a direct impact on people's lives, an ethical imperative that can not be dismissed superficially.