
# Expert Elicitation

\begin{flushright}
\textit{"Expert opinion and judgment enter into the practice of statistical inference and decision-making in numerous ways. Indeed, there is essentially no aspect of scientific investigation in which judgment is not required.."}

— O’Hagan, 2019, p.1
\end{flushright}

::: {.empty}
\vspace{5mm}
:::

In this chapter we will examine expert elicitation, providing a definition and a brief explanation. We will then outline its main challenges, addressing issues related to expertise, estimate quality, cognitive biases, and aggregation methods. This distinction will allow us to introduce the leading protocols for expert elicitation, explaining how structured approaches can improve accuracy and decision-making. We will then discuss the potential application of these methods for establishing a *SESOI*. Finally, we will introduce a tailored implementation of a remote elicitation.

## Why we need expert elicitation

In the previous chapter, we focused on effect size interpretation as a crucial aspect for addressing sample pre-planning and H~1~ specification. Having distinguished between statistical and practical significance, we outlined how the numerical value of the effect size alone does not inform us directly about any theoretically nor practically meaningful implication. To enhance interpretation, we then proposed a method to integrate the plausible effect size and the Smallest Effect Size of Interest (*SESOI*). As explained, the two are complementary, as the plausible effect size informs about the statistical plausibility of our findings while the *SESOI* could give us a precious practical insight. 

As we have already outlined, *SESOI* can be obtained based on different methods, i.e. cost-benefit analyses, anchor based methods and consensus methods. Having already discussed the limitations of the first two in the previous chapter, among these methods, consensus methods stand out as particularly promising for our framework.

Firstly, by relying on consensus methods we may ensure the obtained effect size is linked to its practical or clinical relevance. In practice, clinical relevance is often determined by expert clinicians' judgement, where clinicians must take critical decisions. We argue that, despite their inherent limitations, these experts constitute the best available competence we have. In practice, since relying on experts' judgement has been our reliable approach so far, it represent the more solid ground we have on which to establish our *SESOI*. This is especially true as long as we find a way to systematize said opinions, and until more precise and accurate criteria are established. 
Secondly, consensus methods procedures are well known and extensive literature has already been produced on this subject, making its implementation both feasible and sound.

While at a first glance it may be questionable to rely on experts' judgement, in fields where a value is not known, or decision making is crucial, it is reasonable to rely on experts' opinions [@european2014guidance]. This is even more reasonable since one's subjective opinion can be collected in a well established procedure, which is mostly known as Expert Knowledge Elicitation (EKE) [@o2019expert]. 

In this chapter we will provide an explanation of such methodologies. We will then introduce the protocol we will be using to collect the *SESOI* in chapter four. 

## What is expert elicitation

Expert Knowledge Elicitation (EKE) is a well-established and sound procedure that incorporates expert judgement into formal analyses. The literature on EKE is extensive and covers many fields, including statistics, management sciences, economics, and environmental sciences [@o2019expert].

Generally speaking, _elicitation_ is the process of gathering information needed for a given reason [@european2014efsa]. In EKE, this process refers specifically to obtaining information from one or more experts [@european2014efsa; @o2019expert], and in research context, elicitation is usually used to obtain and formalize expert knowledge in the form of probability distributions or estimates [@o2019expert; @o2006uncertain].

In this section, we will address four interconnected issues in EKE: the matter of expertise, the quality of estimates, cognitive biases, and the aggregation problem. First, the definition and selection of expertise determine the validity of the elicitation process, as only well-qualified experts can provide meaningful judgements. Second, the notion of a "good" estimate is examined through different criteria (accuracy, bias, calibration, and reliability). Third, since research has shown that expert opinion is not immune to systematic distortions, we will adress the matter of biases. Finally, the aggregation problem is discussed, as expert elicitation usually involves multiple contributors but ultimately requires a single output for decision-making. 

### The matter of expertise

The definition of "expert" and "expertise" is a key issue in EKE. Various definitions exist, depending on the purpose, methodology, and underlying theories [@european2014efsa; @burgman2015trusting; @o2006psychology], but there is no clear or universally accepted distinction between lay and expert judgment [@burgman2015trusting]. Even though the debate on this matter is ongoing, when running a study it is essential to clearly specify the type of expertise required and the criteria for expert selection [@european2014efsa]. 
Linked to this issue, an other significant issue revolves around the number of experts involved in the elicitation process. From a theoretical perspective, involving as many experts as possible is advantageous, as it  improves the accuracy of estimates for the variables of interest. However, empirical evidence suggests that including too many experts becomes problematic. While the ideal number depends on the field and study, a common recommendation is to involve a minimum of 5 experts, with an upper limit typically between 8 and 15 [@european2014efsa]. Using too few will limit the diversity of perspectives and the informativeness of the results, while involving too many will add little value to the final estimates, while excessively increasing cost and time requirements [@european2014efsa].

### The matter of estimates 

It is common practice, and almost common sense, to seek expert opinion when facing uncertainty or lack of knowledge on a particular issue. EKE formalizes this reasonable behavior by asking experts estimates for a given quantity, therefore we shall wonder what defines a “good” estimate. There are several criteria which are used to evaluate expert estimates, but the most typically used are accuracy, bias, calibration, and reliability [@burgman2015trusting] :

- _Accuracy_ is how close the expert’s quantitative estimate is to the true value.
- _Bias_ measures a consistent deviation from the true value, in one direction.
- _Calibration_ is a measure of how often the expert’s esteemed intervals contain the true value.
- _Reliability_ addresses the characteristic of the expert, as it reflects the degree to which their estimates are repeatable and stable over time.

Not all of these factors are equally important, and the balance depends on the specific problem at hand, as in some circumstances some criteria could be more relevant than others [@burgman2015trusting].
Although there is an open discussion about how accurate and reliable expert judgements are compared to non-expert opinions, evidence shows that experts generally provide better estimates within their area of expertise. For instance, reliability in EKE is often criticized, as judgements are not very stable over time, which reflects some of the compromises one must accept when using expert elicitation [@burgman2015trusting]. Nevertheless, we rely on expert advice when making decisions in situations where we lack sufficient information, and when it is our best or only source available [@burgman2015trusting]. This is particularly relevant in fields like clinical psychology, where objective benchmarks are often unavailable and researchers often rely heavily on unstructured clinical judgment. In these situations, expert intuition frequently serves as the primary basis for crucial decisions. Formalizing this necessary but subjective judgment through EKE provides a systematic way to reduce arbitrary and naïve conclusions, making it a crucial methodology for robust clinical research. 

### The aggregation problem

As we already mentioned, in most cases, judgements are collected from multiple experts, as it increases the quality of the estimated value, but we typically require a single value as the final output for decision-making. This challenge is referred to as the aggregation problem [@o2019expert].

To address the aggregation problem, two main strategies are commonly employed, the "mathematical aggregation" and the "behavioural aggregation" [@o2019expert].
The first solution is "mathematical aggregation", which involves eliciting individual judgements from each expert and fitting a probability distribution to each of them. These distributions are then combined into a single aggregated distribution by a mathematical formula. Such formula is known as a pooling rule. Several pooling rules exist, and choosing the most appropriate one is the most significant choice when implementing this approach [@o2019expert]. 
The second solution is the behavioural aggregation, which relies on interaction among experts. Firstly, experts discuss their views and reach a consensus. Then, a distribution is fitted to the group’s collective judgment. This method, however, is so sensitive to interpersonal dynamics that may introduce bias. One example could be that dominant personalities can influence group discussions, even unintentionally, compromising the final result [@o2019expert]. One of the most significant risks in behavioural aggregation was first noticed by Janis (1972), as the phenomenon of "groupthink". In this phenomenon, the desire for consensus overrides critical evaluation, leading to sub-optimal decisions.

### The matter of biases

Such a reliance on experts' judgement makes it crucial to understand and consider potential biases that may influence expert decisions. Unstructured or naïve questioning can introduce unwanted cognitive biases, which can affect experts' judegements [@o2006psychology].
Psychological research has identified ways in which superficial or unstandardised questioning can induce cognitive biases in expert judgment [@o2019expert].  Below, we outline some of the most significant ones to provide an overview of the attention this issue has received in the literature:

- **Anchoring:** When asked to provide a numerical estimate, individuals tend to rely heavily on the initial value they consider (the “anchor”), and succesive adjustments remain biased toward that point [@o2019expert].
- **Availability:** Events that are more easily recalled or more memorable are often judged more probable, which can lead to an overestimation of dramatic or recent occurrences [@o2019expert].
- **Range-Frequency:** When the possible values of an uncertain quantity are divided into categories, experts tend to distribute probabilities quite evenly across those categories, regardless of the reasonable likelihoods [@o2019expert].
- **Overconfidence:** Experts often display overconfidence, which may result in confidence intervals that don't include the true value, therefore leading to low calibration values. This could be due to social pressures to demonstrate expertise, or due to the nature of elicited questions, which often involve less routine or familiar quantities. Sadly, habitual heuristics that serve experts well in everyday tasks may not be as effective in these elicitation contexts [@o2019expert].

To reduce the influence of these biases, questions in EKE are carefully designed and preplanned to minimise their impact [@european2014efsa]. To make expert knowledge as objectively as possible, elicitation must be carefully structured. This has led to the development of formal protocols, that are essential to enhance reproducibility and transparency while reducing the influence of biases. We will discuss said protocols in the following section.

## The leading protocols

The decision to adopt either mathematical or behavioural aggregation methods has a direct impact on the conclusione we derive from expert judgements. Moreover, it gives even more importance to structuring proper protocols, which aim to reduce cognitive biases and enhance transparency and reproducibility throughout the process. These protocols differ greatly in their level of formalization, their approach to uncertainty, how they structure expert interaction and the aggregation process itself. Some of these have emerged as the leading ones in the literature. However, it is not possible to determine definitively which protocol is “the best” in terms of accurately capturing experts’ knowledge and beliefs in the form of probability distributions. Even in the rare scenario where the true values of the quantity of interest are eventually revealed, drawing reliable comparisons would require a substantial number of experts, randomly assigned to different protocols, and a large enough number of elicitation tasks across multiple scenarios for appropriate replication and generalization. Nonetheless, the field of expert knowledge elicitation remains a fertile ground for innovative and systematic research efforts to advance its development [@o2019expert].

EFSA recommends the use of three main elicitation protocols [@european2014efsa].

1) The **Cooke protocol** is based on mathematical aggregation[@cooke1991experts]. It follows a structured approach known as the classical model. In this method, before providing estimates for the quantity of interest, experts first provide independent judgments on a set of _seed variables_. These seed variables are ulterior quantities to be asked, related to the nature of the target variables, but whose true value is known. The expert's accuracy in judging the seed variables serves as an indicator of the quality of their estimates for the unknown quantities, as this protocol assigns weights to experts based on their performance on the seed questions, and with these weights individual judgments are combined into a single aggregated distribution. The more precise one's seed estimate, the heavier its weight in the mathematical aggregation [@o2019expert].
2) The **Sheffield protocol** utilizes behavioural aggregation [@oakley2010shelf]. It involves two distinct rounds of expert elicitation. During the first round, experts provide their individual assessments privately to a facilitator. These judgments are then shared and discussed collectively, with the goal of understanding the reasons behind the differences in opinion. Following this discussion, in a second round the group works together to reach a consensus judgment. This consensus is intended to represent the viewpoint of a rational, impartial observer, who should be able to compare objectively the different opinions. The protocol requires a skilled facilitator to guide the process, to ensure to minimize the disturbance of group dynamics and mitigate the biases of the discussions [@o2019expert].
3) The **classic Delphi method** [@rowe1999delphi; @linstone1975delphi], finally, is mostly used to elicit judgments of uncertainty rather than simple point estimates. This approach combines elements of both mathematical and behavioral aggregation, in a particularly flexible and feasible procedure [@brady2015utilizing]. Experts provide their judgments over two or more rounds, with feedback between rounds, to finally generate a collective response. Since anonymity of individual experts is crucial in this approach, there is limited interaction between experts, allowing some sharing of knowledge, while minimizing social biases risk. After the final round, a pooling rule is applied to combine the experts’ distributions into a single aggregated judgment [@o2019expert].

Even though these methods typically require a significant investment of time, effort, and financial resources, more agile and feasible versions have been developed. These include methods like the IDEA protocol [@hemming2018practical], which streamlines expert elicitation through two estimation rounds with intermediate discussion, enabling flexible remote implementation while maintaining methodological rigor.

## Why eliciting a *SESOI*

As already discussed, elicitation protocols sure have many limitation and issues that must be taken into account. Nonetheless, where little information is available and decisions must be made, relying on the most accurate possible judgement we can elicitate from expert could help improve both research and practice within a given field. The fact that their application has extended to fields like clinical psychology further underscores their utility and validity [@jorm2015using; @wu2022application; @sforzini2022delphi]

Guidelines often recommend eliciting values rather than judgments [@hemming2018practical]. But precisely because of the absence of clear guidance for clinicians, it's arguably even more crucial to establish a shared threshold through a trasparent, structured, and deliberative process that minimizes individual biases. The very lack of objectivity of the construct at hand does not weaken the case for expert elicitation, but strengthens it further. 

Moreover, we believe that expert elicitation could prove particularly valuable in the interpretation of effect sizes in psychological research, especially given the absence of clear criteria from either theory or other sources, as we outlined in chapter two and three.

## A tailored procedure for eliciting *SESOI*

Since formal methods for a proper elicitation, such as the IDEA protocol, were not feasible within the constraints of this study regarding time and resources, a pragmatic, direct-to-expert approach was developed. This tailored procedure was designed to quickly capture the collective clinical intuition of practicing therapists, translating their experiential knowledge into a quantifiable benchmark for a minimal, yet meaningful, improvement.

In the following chapter we will detail and implement this tailored elicitation procedure to address the central aim of this thesis: to advance effect size interpretation, within the Neyman-Pearson inferential framework, by providing a principled method for specifying a meaningful alternative hypothesis, and inform decision-making when theoretical guidance is lacking.