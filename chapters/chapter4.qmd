# Methodology

In this chapter, we will apply the aforementioned methodology to integrate the plausible effect size and the smallest effect size of interest (*SESOI*), addressing its challenges and limitations. We will then conduct a power analysis for a preregistered study, informed by the outcomes of the previous steps, as a case study to demonstrate the practical utility and implications of this integrated approach for further research.

## Introduction 

In the previous chapters, we highlighted the conceptual difference between statistical and practical significance, introducing respectively the concepts of the plausible effect size and smallest effect size of interest (*SESOI*). To bridge this gap, we presented a novel methodology to integrate these two approaches, thereby enhancing the interpretation of research findings and theory building in psychology, with a specific focus on clinical psychology.

To emphasize the importance of rigorous and transparent research, we recommend this methodology as a preliminary step for a preregistered study or a registered report. Preregistration is a tool for enhancing research transparency by declaring the research hypotheses, methodology, and analysis plan before data collection [@science; @ummul2021easing]. This practice strengthens inferential validity and leads researchers to conceptualize and critically reason through all study phases prior to its implementation. This a-priori reasoning aligns conceptually with the core aim of this thesis: to help pre-specify an effect size that is both empirically grounded and clinically relevant.

For completeness, we note that while preregistration mitigates issues like HARKing [@wagenmakers2012agenda], its vulnerability to selective reporting might make more rigorous solutions preferable, such as the registered report format [@nosek2014registered; @ummul2021easing], where peer review occurs before data collection, incentivizing researchers to prioritize methodological standards over specific results. However, this approach demands substantial upfront investment and is not always feasible [@ummul2021easing].

Although highly desirable, formal preregistration is not strictly required to implement this methodology. A reasoned and pre-specified effect size always strengthens inference, provides a better theoretical foundation, and enables appropriate study power. 

In this chapter we will move from the theoretical framework to its practical application, as follows. First, we will derive a plausible effect size from the existing literature and a *SESOI* through a process of expert elicitation, as justified in Chapter 3. We will then outline the methodological and conceptual challenges of comparing these two distinct measures, along with our proposed solution. Subsequently, to ground this methodology in a practical context, we will conduct a power analysis for a preregistered study, informed by the outcomes of these initial steps. Finally, the results from this applied methodology will be presented and discussed.

Before detailing these steps we first introduce the primary measure used in our application.

#### The Kessler-10 scale {.unnumbered .unlisted}

To ground our methodology in a concrete example, we will apply it to the Kessler-10 (K-10) [@kessler2002short], a screening tool for psychological distress. Since the SESOI is inherently measure-specific, what constitutes a minimal important change depends entirely on the instrument used. Therefore it is essential to define and justify the chosen measure. 

As psychological distress is an umbrella term encompassing various constructs, such as stress, anxiety, and depression, the K-10 is specifically designed to capture symptoms primarily related to anxiety and depressive disorders. This 10-item self-report questionnaire is widely used in international epidemiological research to identify individuals with likely mental health disorders. Its brevity, strong psychometric properties [@lace2019investigating; @wojujutari2024consistency], and free availability make it a practical tool for both research and primary care settings. Respondents indicate how frequently they experienced each symptom over the past 30 days on a 5-point Likert scale (from 1 = "None of the time" to 5 = "All of the time"). Total scores range from 10 to 50, with higher scores reflecting greater severity of psychological distress.

## Methods and materials
In the following subsections we will detail the specific procedures applied for each phase, in the following order: 

- Meta-analysis: we will select a meta-analysis from literature to determine the plausible effect size.
- Elicitation: we will describe a tailored elicitation procedure applied to derive the smallest effect size of interest (*SESOI*).
- Comparison: we will address the challenges linked to the comparison of these distinct measures and present one possible solution.


### Meta-analysis
We selected a meta-analysis by Madrid et al. (2025) to build up an illustrative case. This recent work synthesizes evidence on digital mental health interventions for university students with mental health difficulties, investigating outcomes across both anxiety and depression domains. 

For the present study, we extracted the effect size for depression outcomes from this meta-analysis to serve as our plausible effect size. This choice aligns with the expert elicitation procedure, which was about the score variation of the K-10 using a clinical vignette featuring a patient with depressive symptoms.

The effect size we extracted was _d_= 0.55

### Elicitation procedure
We will now outline how the survey was constructed, how experts were recruited and data collected.

Since time and resource constraints prevented us from conducting a manualized elicitation, we conducted a tailored elicitation, that could best meet our resources limitation. The elicitation consisted of a survey via e-mail, with a one-shot question to answer. The choice of this non-manualized methodology is considered appropriate within the context of a master's thesis, intended for illustrative purposes only, and serves as a substitute for a proper elicitation process.
The complete elicitation email is reproduced in its original language in Appendix A.

#### Survey structure {.unnumbered .unlisted}
The survey was centered around a detailed clinical vignette depicting a university student with emerging depressive symptoms. The vignette was designed to illustrate a scenario where, after an initial treatment, the patient exhibits a minimal, yet meaningful, functional improvement. Neither length of the treatment nor type of treatment was detailed. This choice may be discussed, but it's our opinion that given the very definition of *SESOI* (the smallest effect size of interest) we are not interested in the time needed to obtained a relevant improvement, nor we are interested in how the improvement is obtained (whether it may be due to a certain treatment, or even spontaneous). Conversely, we are only interested in the score difference, given the relevant change.

Following the vignette, the survey presents the pivotal question: "Given such a relevant change, what difference in the total K-10 score would you expect?" (the original and complete version can be found in the appendix).

For illustrative purpose, we provide a translation of the complete survey above. Should this protocol be used in other contexts, we recommend appropriate adaptation, as the original was administered in Italian.

\vspace{0.3cm}

```{=latex}
\noindent
\rule{\textwidth}{0.5pt}
\vspace{2pt}

\textbf{ELICITATION EMAIL SCRIPT}

\textbf{Subject:} Participation in Master's Thesis Study

\textbf{Body:}

Dear [Dr. X],

\vspace{0.2cm}

Thank you for agreeing to take part in this data collection. The results will be used for a Master's thesis project on methodological research in clinical psychology.

\vspace{0.2cm}

Completion time: \textbf{<5 min}

\vspace{0.2cm}

Our goal is to determine what change in the Kessler-10 scale score indicates a minimal clinically significant improvement, i.e., a first real progress towards better functionality for the patient.

We understand that quantifying your opinion might be difficult, but at this stage of the study, it is essential to rely on the clinical experience of professionals like you.

Please consider the following clinical case as an example.

\vspace{0.2cm}

The patient is a university student, with a family history of depression and a history of bullying during his developmental years. He has recently moved to a new city to begin his studies but is encountering academic difficulties. Noticing that the patient is beginning to show a depressive symptomatology, you administer the Kessler-10.

The patient then undergoes a course of treatment. After the treatment, you detect a minimal improvement in the patient's functioning.

Consequently, you re-administer the Kessler-10.

\vspace{0.2cm}

What total score difference do you expect between the first and second administration?

\vspace{0.2cm}

I expect a difference of [x] points.

If you have any doubts, please still indicate the value you consider most plausible. At this stage of the study, your personal "clinical intuition" is the data we are interested in capturing.

\vspace{0.2cm}

Your responses will be used exclusively for research purposes and will be shared only in aggregate form.

\vspace{0.2cm}

\textbf{We kindly ask you to reply to this email within one week (Wednesday, October 29)}. You will receive a reminder on the closing day.

\vspace{0.2cm}

Thank you for your valuable contribution.

Best regards,

Emanuele Bollini - Master's Degree Student\\
Gianmarco Altoè - Supervising Advisor\\

\vspace{2pt}
\rule{\textwidth}{0.5pt}
```

\vspace{0.3cm}

The methodology was inspired by the final two stages of the IDEA protocol (notably, "estimate" and "aggregate" phases), which we detailed in the previous chapter [@hemming2018practical]. In line with the "estimate" phase, experts were asked to provide their best-guess estimates independently and privately, avoiding group dynamics like groupthink, within the one-week timeframe suggested by the protocol. Following the steps of the "aggregate" phase, the final IDEA phase, the collected responses were mathematically aggregated to derive a group estimate.

#### Pilot test {.unnumbered .unlisted}
To refine the clarity and feasibility of the elicitation instrument, a pilot test was conducted. A sample of 6 master's students in clinical psychology was recruited. After being provided with a description of the K-10 scale, they were sent the survey. Their feedback and responses were used to identify ambiguities and ensure clarity.

#### Ethical {.unnumbered .unlisted}
Since no sensitive data were collected, ethical committee approval was deemed unnecessary. Participants were assured that their individual responses would be presented exclusively in aggregated form.

#### Participants {.unnumbered .unlisted}
A panel of 8 experts in clinical psychology were recruited. Experts were defined as licensed psychotherapists with over 2 years of clinical experience, who administer or used to administer the K-10 in their clinical practice. Recruitment occurred by personal invitations, based on a combination of convenience and availability

#### Data collection {.unnumbered .unlisted}
Upon agreeing to participate in the data collection, participants were sent the questionnaire directly via email, without a preliminary explanatory phase. 
The survey was distributed via email on Wednesday, October 22 at 09:15 to six of the participants, and a second survey was distributed via email on Monday, October 27 at 9:15 to the remaining two (whose email address took longer to collect.)
A reminder was sent on the scheduled deadline day (October 29) at 09:15 for the first survey and on the scheduled deadline day (November 3) for the second round. 

#### Exclusions and interpretation {.unnumbered .unlisted}
Out of 8 experts, 3 responses were excluded as missing data for different reasons. One expert requested additional information, deeming the protocol invalid without it. Despite our clarification, the expert failed to provide a quantitative estimate. One expert raised concerns about the clinical relevance of the parameter proposed in the answer, casting doubt on the reliability of this response. One expert, during the response phase, revealed information that confirmed to have been erroneously included in the panel, not meeting the pre-defined expertise criteria.

Out of five accepted responses, two responses have been discussed. 
One response offered two different estimate under different baseline scenario, proposing 5 under moderate distress and six-to-eight under severe distress condition. Since our clinical vignette did not depicted a severe distress condition, and since its coherent with the definition of a minimal yet meaningful improvement, we took the lower response. One response offered six-to-seven as value. Relying on the same reasoning as the previous case, we considered the lower value. 

Following these methodological decisions, the resulting five values were "5, 5, 5, 5, 6".

#### Aggregation {.unnumbered .unlisted}
Since we are dealing with constructs that cannot be verified, we could not operate techniques such as performance weighting, as described in the previous chapter, which require questions with known answers to calibrate expert weights. This constraint made the use of equal-weight aggregation inevitable, a methodological compromise widely accepted in similar contexts, despite carrying the risk of diluting the contribution of the more accurate experts. Although the choice of aggregation method in such methodologies is often subject to discussion, in this case the mode, median, and mean were all equal to 5. Therefore, we established the value of 5 as the reasonable definitive reference value.

### Comparing *SESOI* and Plausible effect
As outlined in chapter 2, our primary objective is to juxtapose the *SESOI* with the plausible effect size, as this will enable us to conduct a power analysis informed by the comparison of the two indices. A key methodological challenge lies in the inherent difference between the plausible and the *SESOI*, as these two indices are derived from different study designs. In this section we will illustrate the differences between the two and under which assumption we are able to do the comparison. 
 
#### Metanalysis {.unnumbered .unlisted}

The plausible effect size from the meta-analysis [@madrid2025digital] is a between-groups Cohen's *d*. This metric is derived from studies that use a randomized controlled trial (RCT) design, where participants are randomly assigned into separate groups, i.e. a treatment group and a control group. Participants in the control group are given a placebo treatment, while participants in the treatment group are given the actual treatment. After the intervention, the effect size is calculated as follows:

$$
d_{between} = \frac{M_{treat} - M_{control}}{SD_{pooled}}
$$

where $(M_{treat})$ is the average score of the treatment group after the intervention, $M_control$ is the average score of the control group, and $(SD_{pooled})$ is a weighted average of the standard deviations from the two independent groups. In other words, the $(d_{between})$  expresses the post-treatment difference of treatment groups relative to control group, in terms of standard deviations.

#### *SESOI* {.unnumbered .unlisted}

In contrast, the expert-elicited *SESOI* represents a within-subject pre-post change, where a single group of participants is measured both before and after an intervention. The effect size is calculated as follows:

$$
d_{within} = \frac{\bar{X}_{post} - \bar{X}_{pre}}{SD_{diff}}
$$

where $\bar{X}_{post}$ and $\bar{X}_{pre}$ are the mean scores of a single group on a measure (e.g., K-10) after (post) and before (pre) an intervention, and the standard deviation of the difference scores ($SD_{diff}$) is given by: 

$$
SD_{diff} = \sqrt{SD^2_{pre} + SD^2_{post} - 2r \cdot SD_{pre} \cdot SD_{post}}
$$

[@lakens2013calculating; @morris2002combining], being $SD_{pre}$ and $SD_{post}$ the standard deviation of the scores for the single group before (pre) and after (post) the intervention. In essence, it expresses the average pre-post change within a single group relative to the variation of the group itself.

Assuming equal variances at pre and post, $SD_{pre} = SD_{post} = SD$, the expression simplifies to

$$
SD_{diff} = SD\sqrt{2(1-r)}.
$$

Where $r$ is the correlation coefficient between the pre-test and post-test scores for the same individuals. Hence, for an expected mean change of Δ = 5 points:

$$
d_{within}= \frac{\Delta}{SD \sqrt{2(1 - r)}}
$$

Empirical studies on the K-10 in university/community samples typically report SD ≈ 7–9 [@andrews2001interpreting; @sunderland2011estimating]. Therefore, we will consider two reasonable scenarios, choosing 8 and 9 values, avoiding 7, to model conservative scenarios: 

• *SCENARIO A*: If we assume SD = 8 and r = 0.50:

$$
SD_{diff} = 8 \cdot \sqrt{2(1 - 0.50)} = 8, \quad d_{within} = \frac{5}{8} = 0.625.
$$

• *SCENARIO B*: If we assume SD = 9 and r = 0.40:

$$
SD_{diff} = 9 \cdot \sqrt{2(1 - 0.40)} = 9.86, \quad d_{within} = \frac{5}{9.86} \approx 0.51.
$$

Thus, broadening the range to account for plausible variability, the elicited *SESOI* corresponds to a within-subjects effect size of about *d* within ∈ [0.50, 0.65]
under realistic assumptions.

#### Comparison {.unnumbered .unlisted}

The expert-elicited *SESOI* is a within-subject measure, while the design we are planning (an RCT) and the evidence from our meta-analysis are based on between-group comparisons, therefore they are not directly comparable. To inform the power analysis for a standard RCT, the within-subject *SESOI* must be converted into its between-groups equivalent. 

Thus, referring to the previous formula,

$$
d_{between} = \frac{M_{treat} - M_{control}}{SD_{pooled}}
$$

If we make the following assumptions: 

1) randomization ensures baseline equivalence, 
2) the control group shows negligible change over time,
3) the post-test SD is approximately equal to the baseline SD,

then the expected post-test difference between groups is just the expected within-person improvement in the treatment group, i.e. Δ = 5, but now expressed on the post-test SD scale.

Because

$$
SD_{diff} = SD \sqrt{2(1 - r)} \implies SD = \frac{SD_{diff}}{\sqrt{2(1 - r)}}
$$

we can substitute this into the between-groups definition and obtain the key conversion [@morris2002combining]:

$$
d_{between} = d_{within} \cdot \sqrt{2(1 - r)}
$$
This makes explicit that the difference between within-subjects and between-subjects effect sizes is entirely
due to the correlation between pre and post. Following the two scenarios calculated above,

• *SCENARIO A*: If *d* within = 0.625 and r = 0.50:

$$
d_{between} = 0.625 \times \sqrt{2(1 - 0.50)} = 0.625 \times 1 = 0.625.
$$

• *SCENARIO B*: If *d* within = 0.51 and r = 0.40:

$$
d_{between} = 0.51 \times \sqrt{2(1 - 0.40)} = 0.51 \times 1.095 \approx 0.56.
$$

So, under plausible assumptions for K-10 data, widening the interval for plausibility, the between-groups effect size corresponding to the
elicited 5-point change is approximately

$$
d_{between} \approx 0.55\text{-}0.65
$$

The obtained result is notably consistent with the meta-analytic estimate *d* = 0.55 reported in the review on digital mental health interventions for university students, thereby informing us that in this scenario the plausible effect size is also clinically relevant, according to our experts elicitation.

Two important considerations follow from this process. First, we must note that this close alignment is a specific feature of our case, dependent on our particular parameters—the 5-point change defined by experts, the typical K-10 standard deviations, and the pre-post correlations we assumed. Had these elements been different, we would have needed to develop a different rationale for choosing our target effect size. Second, we can be more or less conservative in deciding how close the two values need to be to justify their combined use. This threshold is not fixed and involves a deliberate methodological choice.

## Analysis 

We conducted a sensitivity analysis to assess how sample size requirements varied across different effect sizes (d = 0.50, 0.55, 0.60) and statistical power levels (70%, 80%, 90%), using the "pwr" package [@Champely2020] in R [@Rcore2023]. 

The value *d* = 0.55 represents the direct meta-analytic estimate and the lower bound of our converted *SESOI*, while *d* = 0.60 represents the upper bound of our converted *SESOI*. We also included *d* = 0.50 as a more conservative scenario to illustrate the impact of a slightly smaller, yet still reasonably acceptable, effect.

The effect size range was selected based on the plausible values derived in the previous section. We chose an alpha level of 0.05 and a two-independent-groups design for all power calculations.

The R code of the analysis is included in Appendix B.

## Results

We report the sample size requirements from our sensitivity analysis, which systematically examined how participant numbers vary across different statistical scenarios. Table 4.1 displays required participants per group (total sample size in parentheses) for two-independent-groups t-test with ($\alpha$) = .05, and shows how sample size increases with higher power levels and decrease with larger effect sizes. Each row shows how sample size changes across power levels for a given effect size, while each column shows how sample size varies across effect sizes for a given power level. Values in parentheses represent the total sample size needed across both groups.

| Effect Size | Power 70%   | Power 80%   | Power 90%   |
|-------------|-------------|-------------|-------------|
| d = 0.50    | 51 (102)    | 64 (128)    | 86 (172)    |
| d = 0.55    | 42 (84)     | 53 (106)    | 71 (142)    |
| d = 0.60    | 36 (72)     | 45 (90)     | 60 (120)    |

Table: Participants per group across effect sizes and power levels:

## Discussion

Our power analysis is grounded in the matching procedure between the *SESOI* and the plausible effect size, which in this case indicated that it would be reasonable to think that target effect sizes around _d_ = 0.55 are both empirically supported and clinically meaningful for sample size determination.

This sensitivity analysis gives valuable insights for study pre-planning by quantifying the trade-offs between statistical robustness and practical feasibility, as illustrated in figure 4.1. This approach can help researchers make informed and sound decisions by pre-specifying effect size expectations and understanding their implications for sample size requirements.

![Required sample size per group by power and effect size.](../figures/power.png){width=60% fig-align=center}

As a practical illustration, a researcher adopting the widely accepted power threshold of 80% might pre-specify a conservative target effect size of *d* = 0.50. The application of our methodology provides the justification for this choice, establishing it as both empirically plausible and clinically meaningful. Consequently, the required total sample size would be 128 participants (considering both control and treatment group), a feasible target for many research in psychology.

This approach enables researchers to make informed decisions by pre-specifying effect size expectations and understanding their concrete implications for resource allocation.  It is important to acknowledge that this methodological approach has specific limitations, which we will address in detail in the following chapter.

### Conclusion

In this chapter we detailed the steps of this novel approach, translating the theoretical methodological framework into an operational procedure. Through a concrete case study, a power analysis for a preregistered study, we have shown how integrating a measure of practical significance (the *SESOI*) with one of statistical significance (the plausible effect size) can directly and meaningfully inform research design.

In the next and final chapter, we will summarize the overarching contribution of this work, discuss its methodological limitations in depth, and propose possible directions for future research to refine.
