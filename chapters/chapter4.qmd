# Methodology

In this chapter, we will apply the aforementioned methodology to integrate the plausible effect size and the smallest effect size of interest (SESOI), addressing its challenges and limitations. We will then conduct a power analysis for a preregistered study, informed by the outcomes of the previous steps, as a case study to demonstrate the practical utility and implications of this integrated approach for further research.

## Introduction

In the previous chapters, we highlighted the conceptual difference between statistical and practical significance, introducing respectively the concepts of the plausible effect size and smallest effect size of interest (SESOI). To bridge this gap, we presented a novel methodology to integrate these two approaches, thereby enhancing the interpretation of research findings and theory building in psychology, with a specific focus on clinical psychology.

In this chapter we will move from the theoretical framework to its practical application, as follows. First, we will derive a plausible effect size from the existing literature and a SESOI through a process of expert elicitation, as justified in Chapter 3. We will then outline the methodological and conceptual challenges of comparing these two distinct measures, along with our proposed solution. Subsequently, to ground this methodology in a practical context, we will conduct a power analysis for a preregistered study, informed by the outcomes of these initial steps. Finally, the results from of this applied methodology will be presented and discussed.

## Methods and materials

In the following subsections we will detail the specific procedures apllied for each phase, in the following order: 
- Meta-analysis: we will select a mata-analysis from literature to determine the plausible effect size.
- Elicitation: we will describe a tailored elicitation procedure applied to derive the smallest effect size of interest (SESOI).
- Comparison: we will address the challenges linked to the comparison of these distinct measures and present one possible solution.

### Meta-analysis

*CHAT* --- The plausible effect size will be derived from the meta-analysis --- *CHAT*

The meta-analysis by Madrid et al. (2025) was selected as a foundational case study for our power analysis due to its recency and its considerable congruence with the pre-registered randomized controlled trial (RCT) under investigation [@trial].

Specifically, the meta-analysis synthesizes evidence on digital mental health interventions (DMHIs) for university students with mental health difficulties. A key commonality is that the majority of the included studies features a follow-up period extending not over six months. The pre-registered trial shares critical similarities in terms of its target population (university students experiencing significant distress), intervention type (a digital, transdiagnostic, guided self-help program), study design (randomized controlled trial), primary constructs measured (distress), and temporal framework.

### Elicitation

We will now outline how the survey was constructed, how experts were recruited and data collected.


Since time and resource constraints prevented us from conducting a manualized elicitation, we conducted a tailored elicitation. The elicitation consested of a survey via e-mail, with a one-shot question to answer. The choice of this non-manualized methodology is considered appropriate within the context of a master's thesis, intended for illustrative purposes only, and serves as a substitute for a proper elicitation process.

**Survey structure**: 
The survey was centered around a detailed clinical vignette depicting a university student with emerging depressive symptoms. The vignette was designed to illustrate a scenario where, after an initial treatment, the patient exhibits a minimal, yet meaningful, functional improvement. Neither lenght of the treatment nor type of treatment was detailed. This choice may be discussed, but it's our opinion that  given the very definition of SESOI (the smallest effect size of interest) we are not interested in the time needed to obtained a relevant improvement, nor we are interested in how the improvement is obtained (whether it may be due to a certain treatment, or even spontaneous). Conversly, we are only interested in the score difference, given the relevant change.

Following the vignette, the survey presents the pivotal question: "Given such a relevant change, what difference in the total K-10 score would you expect?" (the original and complete versione can be fount in the appendix)

The methodology was inspired by the final two stages of the IDEA protocol (notably, "estimate" and "aggregate" phases) [@hemming2018practical]. In line with the "estimate" phase, experts were asked to provide their best-guess estimates independently and privately, avoiding group dynamics like groupthink, within the one-week frametime suggested by the protocol. Following the steps of the "aggregate" phase, the final IDEA phase, the collected responses were mathematically aggregated to derive a group estimate.

To refine the clarity and feasibility of the elicitation instrument, a pilot test was conducted. A sample of 6 master's students in clinical psychology was recruited. After being provided with a description of the K-10 scale, they were sent the survey. Their feedback and responses were used to identify ambiguities and ensure clarity.

Since no sensitive data were collected, ethical committee approval was deemed unnecessary. Participants were assured that their individual responses would be presented exclusively in aggregated form.

Upon agreeing to participate in the data collection, participants were sent the questionnaire directly via email, without a preliminary explanatory phase. 

The survey was distributed via email on Wednesday, 22 October at 09:15. A reminder was sent on the scheduled deadline day at 09:15. 

### Partecipants 

The target population for the  elicitation consisted of practicing clinical therapists. The primary eligibility criteria were direct clinical experience and self-reported familiarity with the use of the Kessler-10 scale in their practice. Beyond these criteria, participants were selected based on a combination of convenience and availability.

**the Kessler-10**
The Kessler Psychological Distress Scale (K-10) is a brief, 10-item elf-report questionnaire, a screening measure of nonspecific psychological distress, with a focus on depression and anxiety symptomatology. It's used widely in international epidemiological research to detect mental health dysfunction. Its brevity, robust psychometric properties  [@lace2019investigating; @wojujutari2024consistency], and freely available use, make it a widely used tool in both research and primary care settings for screening and monitoring. Respondents rate how frequently they experienced each symptom in the last 30 days on a 5-point scale (ranging from 1 = "None of the time" to 5 = "All of the time"). The total score ranges from 10 to 50, where higher scores indicate greater levels of psychological distress. 

*CHAT* --- A panel of [N] experts in [specific field, e.g., clinical psychology, intervention research] will be recruited. Experts will be defined as [e.g., researchers with more than X publications in the field, licensed practitioners with over Y years of experience]. Recruitment will occur via [e.g., professional association mailing lists, personal invitations]. --- *CHAT*

### Comparison

*CHAT* --- The primary objective is to juxtapose the SESOI (derived from Phase 1) with the PES (derived from Phase 2) to answer the following research questions:

Does the plausible interval for the true effect size encompass the smallest effect size of interest?

What proportion of the plausible effect size distribution lies above the SESOI threshold?


A key methodological challenge in this endeavor lies in the inherent dissimilarity between the PES and the SESOI. The former is typically a between-groups contrast (e.g., treatment vs. control), while the latter is conceptually a within-person change. A direct comparison is therefore not immediately feasible. Furthermore, the elicitation of the SESOI is intentionally focused on a within-person improvement without reference to a control group, as this mirrors the information available to clinicians in their daily practice and thus provides a more ecologically valid estimate of a meaningful change. This thesis will explicitly address this challenge by developing a framework to render these two distinct measures comparable. --- *CHAT*

To ensure comparability, the two indices were converted to a common metric.

## Analysis 

*CHAT* --- To illustrate the practical utility of our integrative approach, we will conclude with a design analysis (Altoè et al., 2020; Gelman & Carlin, 2014). This analysis will demonstrate how the findings from the comparative analysis can inform the design of future studies.

We will calculate the required sample size for a hypothetical two-group randomized controlled trial (RCT) aiming to detect an effect. *CHAT*

## Results

## Discussion

- By eliciting only point estimates rather than uncertainty intervals, this approach does not capture their confidence levels and may reinforce overconfidence bias.
- Dal punto di vista operativo, la natura non verificabile del costrutto preclude l’applicazione di tecniche come il “performance weighting” (come descritto nel capitolo precedente), che richiedono domande a risposta nota per calibrare il peso degli esperti. Ciò ha reso inevitabile il ricorso a un’aggregazione a pesi uguali, un compromesso metodologico ampiamente accettato in contesti simili, sebbene comporti il rischio di diluire il contributo degli esperti più accurati.

To our knowledge, there are no precedents in the literature for elicitations of this type. This meant we were unaware of the potential associated problems.

We found that one challenge for clinicians is shifting their focus from "improvement given the treatment" to "score variation given the improvement."

Difficulty in finding clinicians with specific expertise in using a particular scale.

A key challenge was the inherent circularity in defining the "minimally important change" for expert elicitation. To operationalize the concept, we used a clinical vignette depicting minimal but meaningful functional improvement and asked experts to quantify the expected Kessler-10 score change. While necessary, this approach relied on clinicians' individual interpretation, potentially introducing more variability. This underscores the importance of eliciting multiple expert judgments to capture diverse clinical perspectives.

### Closing morale 

---> The primary contribution of this thesis lies not in a specific empirical finding, but in the development and illustration of this procedural framework itself. The ultimate aim is to provide a replicable template for enhancing the interpretation and planning of psychological studies.
This chapter details the steps of this novel approach—from the elicitation of a practical threshold to its comparison with a literature-based benchmark—and concludes with a practical demonstration of how such an integration can inform research design. 

